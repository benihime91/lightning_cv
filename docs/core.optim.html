---

title: Optimizers


keywords: fastai
sidebar: home_sidebar

summary: "Collection of usefull `Optimizers`"
description: "Collection of usefull `Optimizers`"
nb_path: "nbs/00c_core.optim.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/00c_core.optim.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Ranger</strong></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Ranger" class="doc_header"><code>Ranger</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/optim.py#L19" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Ranger</code>(<strong><code>params</code></strong>:<code>Iterable</code>[<code>T_co</code>], <strong><code>betas</code></strong>:<code>Tuple</code>[<code>float</code>, <code>float</code>]=<em><code>(0.95, 0.999)</code></em>, <strong><code>eps</code></strong>:<code>float</code>=<em><code>1e-05</code></em>, <strong><code>k</code></strong>:<code>int</code>=<em><code>6</code></em>, <strong><code>alpha</code></strong>:<code>float</code>=<em><code>0.5</code></em>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>weight_decay</code></strong>=<em><code>0</code></em>)</p>
</blockquote>
<p>Convenience method for <code>Lookahead</code> with <code>RAdam</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="f66c7d7f-9fbb-4306-9027-359a717a66d4"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#f66c7d7f-9fbb-4306-9027-359a717a66d4');

            setTimeout(function() {
                var nbb_cell_id = 7;
                var nbb_unformatted_code = "# export\n@delegates(RAdam)\ndef Ranger(\n    params: Iterable,\n    betas: Tuple[float, float] = (0.95, 0.999),\n    eps: float = 1e-5,\n    k: int = 6,\n    alpha: float = 0.5,\n    **kwargs\n):\n    \"Convenience method for `Lookahead` with `RAdam`\"\n    return Lookahead(RAdam(params, betas=betas, eps=eps, **kwargs), alpha=alpha, k=k)";
                var nbb_formatted_code = "# export\n@delegates(RAdam)\ndef Ranger(\n    params: Iterable,\n    betas: Tuple[float, float] = (0.95, 0.999),\n    eps: float = 1e-5,\n    k: int = 6,\n    alpha: float = 0.5,\n    **kwargs\n):\n    \"Convenience method for `Lookahead` with `RAdam`\"\n    return Lookahead(RAdam(params, betas=betas, eps=eps, **kwargs), alpha=alpha, k=k)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Ranger with Gradient Centralization</strong></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RangerGC" class="doc_header"><code>class</code> <code>RangerGC</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/optim.py#L32" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RangerGC</code>(<strong><code>params</code></strong>:<code>Iterable</code>[<code>T_co</code>], <strong><code>lr</code></strong>:<code>float</code>=<em><code>0.001</code></em>, <strong><code>alpha</code></strong>:<code>float</code>=<em><code>0.5</code></em>, <strong><code>k</code></strong>:<code>int</code>=<em><code>6</code></em>, <strong><code>N_sma_threshhold</code></strong>:<code>int</code>=<em><code>5</code></em>, <strong><code>betas</code></strong>:<code>Tuple</code>[<code>float</code>, <code>float</code>]=<em><code>(0.95, 0.999)</code></em>, <strong><code>eps</code></strong>:<code>float</code>=<em><code>1e-05</code></em>, <strong><code>weight_decay</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0</code></em>, <strong><code>use_gc</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>gc_conv_only</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.
From - <a href="https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py">https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="e738fd26-a687-49f4-a1e8-ce29379ce73f"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#e738fd26-a687-49f4-a1e8-ce29379ce73f');

            setTimeout(function() {
                var nbb_cell_id = 8;
                var nbb_unformatted_code = "# export\nclass RangerGC(Optimizer):\n    \"\"\"\n    Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.\n    From - https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr: float = 1e-3,\n        alpha: float = 0.5,\n        k: int = 6,\n        N_sma_threshhold: int = 5,\n        betas: Tuple[float, float] = (0.95, 0.999),\n        eps: float = 1e-5,\n        weight_decay: Union[float, int] = 0,\n        use_gc: bool = True,\n        gc_conv_only: bool = False,\n    ):\n\n        # parameter checks\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f\"Invalid slow update rate: {alpha}\")\n        if not 1 <= k:\n            raise ValueError(f\"Invalid lookahead steps: {k}\")\n        if not lr > 0:\n            raise ValueError(f\"Invalid Learning Rate: {lr}\")\n        if not eps > 0:\n            raise ValueError(f\"Invalid eps: {eps}\")\n\n        # prep defaults and init torch.optim base\n        defaults = dict(\n            lr=lr,\n            alpha=alpha,\n            k=k,\n            step_counter=0,\n            betas=betas,\n            N_sma_threshhold=N_sma_threshhold,\n            eps=eps,\n            weight_decay=weight_decay,\n        )\n        super().__init__(params, defaults)\n\n        # adjustable threshold\n        self.N_sma_threshhold = N_sma_threshhold\n\n        # look ahead params\n\n        self.alpha = alpha\n        self.k = k\n\n        # radam buffer for state\n        self.radam_buffer = [[None, None, None] for ind in range(10)]\n\n        # gc on or off\n        self.use_gc = use_gc\n\n        # level of gradient centralization\n        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n\n    def __setstate__(self, state):\n        print(\"set state called\")\n        super(Ranger, self).__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n\n        if closure is not None:\n            loss = closure()\n\n        # Evaluate averages and grad, update param tensors\n        for group in self.param_groups:\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \"Ranger optimizer does not support sparse gradients\"\n                    )\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n\n                    # look ahead weight storage now in state dict\n                    state[\"slow_buffer\"] = torch.empty_like(p.data)\n                    state[\"slow_buffer\"].copy_(p.data)\n\n                else:\n                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n\n                # begin computations\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                # GC operation for Conv layers and FC layers\n                if grad.dim() > self.gc_gradient_threshold:\n                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n\n                state[\"step\"] += 1\n\n                # compute variance mov avg\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                # compute mean moving avg\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                buffered = self.radam_buffer[int(state[\"step\"] % 10)]\n\n                if state[\"step\"] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\"step\"]\n                    beta2_t = beta2 ** state[\"step\"]\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n                    if N_sma > self.N_sma_threshhold:\n                        step_size = math.sqrt(\n                            (1 - beta2_t)\n                            * (N_sma - 4)\n                            / (N_sma_max - 4)\n                            * (N_sma - 2)\n                            / N_sma\n                            * N_sma_max\n                            / (N_sma_max - 2)\n                        ) / (1 - beta1 ** state[\"step\"])\n                    else:\n                        step_size = 1.0 / (1 - beta1 ** state[\"step\"])\n                    buffered[2] = step_size\n\n                if group[\"weight_decay\"] != 0:\n                    p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n\n                # apply lr\n                if N_sma > self.N_sma_threshhold:\n                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n                    p_data_fp32.addcdiv_(-step_size * group[\"lr\"], exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size * group[\"lr\"], exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n                # integrated look ahead...\n                # we do it at the param level instead of group level\n                if state[\"step\"] % group[\"k\"] == 0:\n                    # get access to slow param tensor\n                    slow_p = state[\"slow_buffer\"]\n                    # (fast weights - slow weights) * alpha\n                    slow_p.add_(self.alpha, p.data - slow_p)\n                    # copy interpolated weights to RAdam param tensor\n                    p.data.copy_(slow_p)\n\n        return loss";
                var nbb_formatted_code = "# export\nclass RangerGC(Optimizer):\n    \"\"\"\n    Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.\n    From - https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr: float = 1e-3,\n        alpha: float = 0.5,\n        k: int = 6,\n        N_sma_threshhold: int = 5,\n        betas: Tuple[float, float] = (0.95, 0.999),\n        eps: float = 1e-5,\n        weight_decay: Union[float, int] = 0,\n        use_gc: bool = True,\n        gc_conv_only: bool = False,\n    ):\n\n        # parameter checks\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f\"Invalid slow update rate: {alpha}\")\n        if not 1 <= k:\n            raise ValueError(f\"Invalid lookahead steps: {k}\")\n        if not lr > 0:\n            raise ValueError(f\"Invalid Learning Rate: {lr}\")\n        if not eps > 0:\n            raise ValueError(f\"Invalid eps: {eps}\")\n\n        # prep defaults and init torch.optim base\n        defaults = dict(\n            lr=lr,\n            alpha=alpha,\n            k=k,\n            step_counter=0,\n            betas=betas,\n            N_sma_threshhold=N_sma_threshhold,\n            eps=eps,\n            weight_decay=weight_decay,\n        )\n        super().__init__(params, defaults)\n\n        # adjustable threshold\n        self.N_sma_threshhold = N_sma_threshhold\n\n        # look ahead params\n\n        self.alpha = alpha\n        self.k = k\n\n        # radam buffer for state\n        self.radam_buffer = [[None, None, None] for ind in range(10)]\n\n        # gc on or off\n        self.use_gc = use_gc\n\n        # level of gradient centralization\n        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n\n    def __setstate__(self, state):\n        print(\"set state called\")\n        super(Ranger, self).__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n\n        if closure is not None:\n            loss = closure()\n\n        # Evaluate averages and grad, update param tensors\n        for group in self.param_groups:\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \"Ranger optimizer does not support sparse gradients\"\n                    )\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n\n                    # look ahead weight storage now in state dict\n                    state[\"slow_buffer\"] = torch.empty_like(p.data)\n                    state[\"slow_buffer\"].copy_(p.data)\n\n                else:\n                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n\n                # begin computations\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                # GC operation for Conv layers and FC layers\n                if grad.dim() > self.gc_gradient_threshold:\n                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n\n                state[\"step\"] += 1\n\n                # compute variance mov avg\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                # compute mean moving avg\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                buffered = self.radam_buffer[int(state[\"step\"] % 10)]\n\n                if state[\"step\"] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\"step\"]\n                    beta2_t = beta2 ** state[\"step\"]\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n                    if N_sma > self.N_sma_threshhold:\n                        step_size = math.sqrt(\n                            (1 - beta2_t)\n                            * (N_sma - 4)\n                            / (N_sma_max - 4)\n                            * (N_sma - 2)\n                            / N_sma\n                            * N_sma_max\n                            / (N_sma_max - 2)\n                        ) / (1 - beta1 ** state[\"step\"])\n                    else:\n                        step_size = 1.0 / (1 - beta1 ** state[\"step\"])\n                    buffered[2] = step_size\n\n                if group[\"weight_decay\"] != 0:\n                    p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n\n                # apply lr\n                if N_sma > self.N_sma_threshhold:\n                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n                    p_data_fp32.addcdiv_(-step_size * group[\"lr\"], exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size * group[\"lr\"], exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n                # integrated look ahead...\n                # we do it at the param level instead of group level\n                if state[\"step\"] % group[\"k\"] == 0:\n                    # get access to slow param tensor\n                    slow_p = state[\"slow_buffer\"]\n                    # (fast weights - slow weights) * alpha\n                    slow_p.add_(self.alpha, p.data - slow_p)\n                    # copy interpolated weights to RAdam param tensor\n                    p.data.copy_(slow_p)\n\n        return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>SGDP</strong></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SGDP" class="doc_header"><code>class</code> <code>SGDP</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/optim.py#L196" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SGDP</code>(<strong><code>params</code></strong>:<code>Iterable</code>[<code>T_co</code>], <strong><code>lr</code></strong>=<em><code>&lt;required parameter&gt;</code></em>, <strong><code>momentum</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0</code></em>, <strong><code>dampening</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0</code></em>, <strong><code>weight_decay</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0</code></em>, <strong><code>nesterov</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>eps</code></strong>:<code>float</code>=<em><code>1e-08</code></em>, <strong><code>delta</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>wd_ratio</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0.1</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>SGDP Optimizer Implementation copied from <a href="https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py">https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="58291992-992c-49f5-ba0d-07810e2db3ce"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#58291992-992c-49f5-ba0d-07810e2db3ce');

            setTimeout(function() {
                var nbb_cell_id = 9;
                var nbb_unformatted_code = "# export\nclass SGDP(Optimizer):\n    \"SGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py\"\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr=required,\n        momentum: Union[float, int] = 0,\n        dampening: Union[float, int] = 0,\n        weight_decay: Union[float, int] = 0,\n        nesterov: bool = False,\n        eps: float = 1e-8,\n        delta: float = 0.1,\n        wd_ratio: Union[float, int] = 0.1,\n    ):\n\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n            eps=eps,\n            delta=delta,\n            wd_ratio=wd_ratio,\n        )\n        super(SGDP, self).__init__(params, defaults)\n\n    def _channel_view(self, x):\n        return x.view(x.size(0), -1)\n\n    def _layer_view(self, x):\n        return x.view(1, -1)\n\n    def _cosine_similarity(self, x, y, eps, view_func):\n        x = view_func(x)\n        y = view_func(y)\n\n        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n\n    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n        wd = 1\n        expand_size = [-1] + [1] * (len(p.shape) - 1)\n        for view_func in [self._channel_view, self._layer_view]:\n\n            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n\n            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n                wd = wd_ratio\n\n                return perturb, wd\n\n        return perturb, wd\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            momentum = group[\"momentum\"]\n            dampening = group[\"dampening\"]\n            nesterov = group[\"nesterov\"]\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"momentum\"] = torch.zeros_like(p.data)\n\n                # SGD\n                buf = state[\"momentum\"]\n                buf.mul_(momentum).add_(grad, alpha=1 - dampening)\n                if nesterov:\n                    d_p = grad + momentum * buf\n                else:\n                    d_p = buf\n\n                # Projection\n                wd_ratio = 1\n                if len(p.shape) > 1:\n                    d_p, wd_ratio = self._projection(\n                        p, grad, d_p, group[\"delta\"], group[\"wd_ratio\"], group[\"eps\"]\n                    )\n\n                # Weight decay\n                if group[\"weight_decay\"] > 0:\n                    p.data.mul_(\n                        1\n                        - group[\"lr\"]\n                        * group[\"weight_decay\"]\n                        * wd_ratio\n                        / (1 - momentum)\n                    )\n\n                # Step\n                p.data.add_(d_p, alpha=-group[\"lr\"])\n\n        return loss";
                var nbb_formatted_code = "# export\nclass SGDP(Optimizer):\n    \"SGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py\"\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr=required,\n        momentum: Union[float, int] = 0,\n        dampening: Union[float, int] = 0,\n        weight_decay: Union[float, int] = 0,\n        nesterov: bool = False,\n        eps: float = 1e-8,\n        delta: float = 0.1,\n        wd_ratio: Union[float, int] = 0.1,\n    ):\n\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n            eps=eps,\n            delta=delta,\n            wd_ratio=wd_ratio,\n        )\n        super(SGDP, self).__init__(params, defaults)\n\n    def _channel_view(self, x):\n        return x.view(x.size(0), -1)\n\n    def _layer_view(self, x):\n        return x.view(1, -1)\n\n    def _cosine_similarity(self, x, y, eps, view_func):\n        x = view_func(x)\n        y = view_func(y)\n\n        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n\n    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n        wd = 1\n        expand_size = [-1] + [1] * (len(p.shape) - 1)\n        for view_func in [self._channel_view, self._layer_view]:\n\n            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n\n            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n                wd = wd_ratio\n\n                return perturb, wd\n\n        return perturb, wd\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            momentum = group[\"momentum\"]\n            dampening = group[\"dampening\"]\n            nesterov = group[\"nesterov\"]\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"momentum\"] = torch.zeros_like(p.data)\n\n                # SGD\n                buf = state[\"momentum\"]\n                buf.mul_(momentum).add_(grad, alpha=1 - dampening)\n                if nesterov:\n                    d_p = grad + momentum * buf\n                else:\n                    d_p = buf\n\n                # Projection\n                wd_ratio = 1\n                if len(p.shape) > 1:\n                    d_p, wd_ratio = self._projection(\n                        p, grad, d_p, group[\"delta\"], group[\"wd_ratio\"], group[\"eps\"]\n                    )\n\n                # Weight decay\n                if group[\"weight_decay\"] > 0:\n                    p.data.mul_(\n                        1\n                        - group[\"lr\"]\n                        * group[\"weight_decay\"]\n                        * wd_ratio\n                        / (1 - momentum)\n                    )\n\n                # Step\n                p.data.add_(d_p, alpha=-group[\"lr\"])\n\n        return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>AdamP</strong></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AdamP" class="doc_header"><code>class</code> <code>AdamP</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/optim.py#L303" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AdamP</code>(<strong><code>params</code></strong>:<code>Iterable</code>[<code>T_co</code>], <strong><code>lr</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0.001</code></em>, <strong><code>betas</code></strong>:<code>Tuple</code>[<code>float</code>, <code>float</code>]=<em><code>(0.9, 0.999)</code></em>, <strong><code>eps</code></strong>:<code>float</code>=<em><code>1e-08</code></em>, <strong><code>weight_decay</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0</code></em>, <strong><code>delta</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>wd_ratio</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>nesterov</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>AdamP Optimizer Implementation copied from <a href="https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py">https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="d9c79ff4-4b61-4ea6-8f67-f3703c1fe283"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#d9c79ff4-4b61-4ea6-8f67-f3703c1fe283');

            setTimeout(function() {
                var nbb_cell_id = 10;
                var nbb_unformatted_code = "# export\nclass AdamP(Optimizer):\n    \"AdamP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py\"\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr: Union[float, int] = 1e-3,\n        betas: Tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: Union[float, int] = 0,\n        delta: float = 0.1,\n        wd_ratio: float = 0.1,\n        nesterov: bool = False,\n    ):\n\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            delta=delta,\n            wd_ratio=wd_ratio,\n            nesterov=nesterov,\n        )\n\n        super(AdamP, self).__init__(params, defaults)\n\n    def _channel_view(self, x):\n        return x.view(x.size(0), -1)\n\n    def _layer_view(self, x):\n        return x.view(1, -1)\n\n    def _cosine_similarity(self, x, y, eps, view_func):\n        x = view_func(x)\n        y = view_func(y)\n\n        x_norm = x.norm(dim=1).add_(eps)\n        y_norm = y.norm(dim=1).add_(eps)\n        dot = (x * y).sum(dim=1)\n\n        return dot.abs() / x_norm / y_norm\n\n    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n        wd = 1\n        expand_size = [-1] + [1] * (len(p.shape) - 1)\n        for view_func in [self._channel_view, self._layer_view]:\n\n            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n\n            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n                wd = wd_ratio\n\n                return perturb, wd\n\n        return perturb, wd\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n\n                grad = p.grad.data\n                beta1, beta2 = group[\"betas\"]\n                nesterov = group[\"nesterov\"]\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n\n                # Adam\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n\n                state[\"step\"] += 1\n                bias_correction1 = 1 - beta1 ** state[\"step\"]\n                bias_correction2 = 1 - beta2 ** state[\"step\"]\n\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(\n                    group[\"eps\"]\n                )\n                step_size = group[\"lr\"] / bias_correction1\n\n                if nesterov:\n                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\n                else:\n                    perturb = exp_avg / denom\n\n                # Projection\n                wd_ratio = 1\n                if len(p.shape) > 1:\n                    perturb, wd_ratio = self._projection(\n                        p,\n                        grad,\n                        perturb,\n                        group[\"delta\"],\n                        group[\"wd_ratio\"],\n                        group[\"eps\"],\n                    )\n\n                # Weight decay\n                if group[\"weight_decay\"] > 0:\n                    p.data.mul_(1 - group[\"lr\"] * group[\"weight_decay\"] * wd_ratio)\n\n                # Step\n                p.data.add_(-step_size, perturb)\n\n        return loss";
                var nbb_formatted_code = "# export\nclass AdamP(Optimizer):\n    \"AdamP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py\"\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr: Union[float, int] = 1e-3,\n        betas: Tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: Union[float, int] = 0,\n        delta: float = 0.1,\n        wd_ratio: float = 0.1,\n        nesterov: bool = False,\n    ):\n\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            delta=delta,\n            wd_ratio=wd_ratio,\n            nesterov=nesterov,\n        )\n\n        super(AdamP, self).__init__(params, defaults)\n\n    def _channel_view(self, x):\n        return x.view(x.size(0), -1)\n\n    def _layer_view(self, x):\n        return x.view(1, -1)\n\n    def _cosine_similarity(self, x, y, eps, view_func):\n        x = view_func(x)\n        y = view_func(y)\n\n        x_norm = x.norm(dim=1).add_(eps)\n        y_norm = y.norm(dim=1).add_(eps)\n        dot = (x * y).sum(dim=1)\n\n        return dot.abs() / x_norm / y_norm\n\n    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n        wd = 1\n        expand_size = [-1] + [1] * (len(p.shape) - 1)\n        for view_func in [self._channel_view, self._layer_view]:\n\n            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n\n            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n                wd = wd_ratio\n\n                return perturb, wd\n\n        return perturb, wd\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n\n                grad = p.grad.data\n                beta1, beta2 = group[\"betas\"]\n                nesterov = group[\"nesterov\"]\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n\n                # Adam\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n\n                state[\"step\"] += 1\n                bias_correction1 = 1 - beta1 ** state[\"step\"]\n                bias_correction2 = 1 - beta2 ** state[\"step\"]\n\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(\n                    group[\"eps\"]\n                )\n                step_size = group[\"lr\"] / bias_correction1\n\n                if nesterov:\n                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\n                else:\n                    perturb = exp_avg / denom\n\n                # Projection\n                wd_ratio = 1\n                if len(p.shape) > 1:\n                    perturb, wd_ratio = self._projection(\n                        p,\n                        grad,\n                        perturb,\n                        group[\"delta\"],\n                        group[\"wd_ratio\"],\n                        group[\"eps\"],\n                    )\n\n                # Weight decay\n                if group[\"weight_decay\"] > 0:\n                    p.data.mul_(1 - group[\"lr\"] * group[\"weight_decay\"] * wd_ratio)\n\n                # Step\n                p.data.add_(-step_size, perturb)\n\n        return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>RMSpropTF from <code>timm</code></strong></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RMSpropTF" class="doc_header"><code>class</code> <code>RMSpropTF</code><a href="timm/optim/rmsprop_tf.py#L14" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RMSpropTF</code>(<strong><code>params</code></strong>, <strong><code>lr</code></strong>=<em><code>0.01</code></em>, <strong><code>alpha</code></strong>=<em><code>0.9</code></em>, <strong><code>eps</code></strong>=<em><code>1e-10</code></em>, <strong><code>weight_decay</code></strong>=<em><code>0</code></em>, <strong><code>momentum</code></strong>=<em><code>0.0</code></em>, <strong><code>centered</code></strong>=<em><code>False</code></em>, <strong><code>decoupled_decay</code></strong>=<em><code>False</code></em>, <strong><code>lr_in_momentum</code></strong>=<em><code>True</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Implements RMSprop algorithm (TensorFlow style epsilon)</p>
<p>NOTE: This is a direct cut-and-paste of PyTorch RMSprop with eps applied before sqrt
and a few other modifications to closer match Tensorflow for matching hyper-params.</p>
<p>Noteworthy changes include:</p>
<ol>
<li>Epsilon applied inside square-root</li>
<li>square_avg initialized to ones</li>
<li>LR scaling of update accumulated in momentum buffer</li>
</ol>
<p>Proposed by G. Hinton in his
<code>course &lt;http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&gt;</code>_.</p>
<p>The centered version first appears in <code>Generating Sequences
With Recurrent Neural Networks &lt;https://arxiv.org/pdf/1308.0850v5.pdf&gt;</code>_.</p>
<p>Arguments:
    params (iterable): iterable of parameters to optimize or dicts defining
        parameter groups
    lr (float, optional): learning rate (default: 1e-2)
    momentum (float, optional): momentum factor (default: 0)
    alpha (float, optional): smoothing (decay) constant (default: 0.9)
    eps (float, optional): term added to the denominator to improve
        numerical stability (default: 1e-10)
    centered (bool, optional) : if <code>True</code>, compute the centered RMSProp,
        the gradient is normalized by an estimation of its variance
    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
    decoupled_decay (bool, optional): decoupled weight decay as per <a href="https://arxiv.org/abs/1711.05101">https://arxiv.org/abs/1711.05101</a>
    lr_in_momentum (bool, optional): learning rate scaling is included in the momentum buffer
        update as per defaults in Tensorflow</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="OPTIM_REGISTERY">OPTIM_REGISTERY<a class="anchor-link" href="#OPTIM_REGISTERY"> </a></h2><blockquote><p><code>Registery</code> of Optimizers</p>
</blockquote>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="ba35142a-09f6-4664-a361-eb47887108cf"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#ba35142a-09f6-4664-a361-eb47887108cf');

            setTimeout(function() {
                var nbb_cell_id = 13;
                var nbb_unformatted_code = "# export\nOPTIM_REGISTERY = Registry(\"OPTIMIZERS\")\nOPTIM_REGISTERY.__doc__ = \"Registery of Optimizers\"";
                var nbb_formatted_code = "# export\nOPTIM_REGISTERY = Registry(\"OPTIMIZERS\")\nOPTIM_REGISTERY.__doc__ = \"Registery of Optimizers\"";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="cfc690ff-0b1a-407e-b5a2-2110fff2a244"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#cfc690ff-0b1a-407e-b5a2-2110fff2a244');

            setTimeout(function() {
                var nbb_cell_id = 14;
                var nbb_unformatted_code = "# export\n# Register common optimizers\nOPTIM_REGISTERY.register(SGD)\nOPTIM_REGISTERY.register(SGDP)\nOPTIM_REGISTERY.register(Adam)\nOPTIM_REGISTERY.register(AdamW)\nOPTIM_REGISTERY.register(AdamP)\nOPTIM_REGISTERY.register(Ranger)\nOPTIM_REGISTERY.register(RangerGC)\nOPTIM_REGISTERY.register(RMSpropTF)";
                var nbb_formatted_code = "# export\n# Register common optimizers\nOPTIM_REGISTERY.register(SGD)\nOPTIM_REGISTERY.register(SGDP)\nOPTIM_REGISTERY.register(Adam)\nOPTIM_REGISTERY.register(AdamW)\nOPTIM_REGISTERY.register(AdamP)\nOPTIM_REGISTERY.register(Ranger)\nOPTIM_REGISTERY.register(RangerGC)\nOPTIM_REGISTERY.register(RMSpropTF)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Registry of OPTIMIZERS:
╒═══════════╤═══════════════════════════════════════════╕
│ Names     │ Objects                                   │
╞═══════════╪═══════════════════════════════════════════╡
│ SGD       │ &lt;class &#39;torch.optim.sgd.SGD&#39;&gt;             │
├───────────┼───────────────────────────────────────────┤
│ SGDP      │ &lt;class &#39;__main__.SGDP&#39;&gt;                   │
├───────────┼───────────────────────────────────────────┤
│ Adam      │ &lt;class &#39;torch.optim.adam.Adam&#39;&gt;           │
├───────────┼───────────────────────────────────────────┤
│ AdamW     │ &lt;class &#39;torch.optim.adamw.AdamW&#39;&gt;         │
├───────────┼───────────────────────────────────────────┤
│ AdamP     │ &lt;class &#39;__main__.AdamP&#39;&gt;                  │
├───────────┼───────────────────────────────────────────┤
│ Ranger    │ &lt;function Ranger at 0x7fd784f96040&gt;       │
├───────────┼───────────────────────────────────────────┤
│ RangerGC  │ &lt;class &#39;__main__.RangerGC&#39;&gt;               │
├───────────┼───────────────────────────────────────────┤
│ RMSpropTF │ &lt;class &#39;timm.optim.rmsprop_tf.RMSpropTF&#39;&gt; │
╘═══════════╧═══════════════════════════════════════════╛</pre>
</div>

</div>

<div class="output_area">




<div id="c67223ad-e325-4776-bc8a-21b9d4a39159"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#c67223ad-e325-4776-bc8a-21b9d4a39159');

            setTimeout(function() {
                var nbb_cell_id = 15;
                var nbb_unformatted_code = "# hide-input\nOPTIM_REGISTERY";
                var nbb_formatted_code = "# hide-input\nOPTIM_REGISTERY";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="create_optimizer" class="doc_header"><code>create_optimizer</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/optim.py#L444" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>create_optimizer</code>(<strong><code>params</code></strong>:<code>Iterable</code>[<code>T_co</code>], <strong><code>cfg</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Instante an optimizer from <code>OPTIM_REGISTRY</code> given <code>params</code> with lightning_cv <code>cfg</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="05ee518a-6d48-48c3-8c6e-f383483bd8e1"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#05ee518a-6d48-48c3-8c6e-f383483bd8e1');

            setTimeout(function() {
                var nbb_cell_id = 16;
                var nbb_unformatted_code = "# export\ndef create_optimizer(params: Iterable, cfg=None):\n    \"Instante an optimizer from `OPTIM_REGISTRY` given `params` with lightning_cv `cfg`\"\n    opt_cls = OPTIM_REGISTERY.get(cfg.OPTIMIZER.NAME)\n    opt = opt_cls(params=params, **cfg.OPTIMIZER.ARGUMENTS)\n    return opt";
                var nbb_formatted_code = "# export\ndef create_optimizer(params: Iterable, cfg=None):\n    \"Instante an optimizer from `OPTIM_REGISTRY` given `params` with lightning_cv `cfg`\"\n    opt_cls = OPTIM_REGISTERY.get(cfg.OPTIMIZER.NAME)\n    opt = opt_cls(params=params, **cfg.OPTIMIZER.ARGUMENTS)\n    return opt";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Creating an Optimizer using <a href="/lightning_cv/core.optim.html#create_optimizer"><code>create_optimizer</code></a> from LightningCv config -</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">lightning_cv.config</span> <span class="kn">import</span> <span class="n">get_cfg</span>
<span class="kn">from</span> <span class="nn">omegaconf</span> <span class="kn">import</span> <span class="n">DictConfig</span><span class="p">,</span> <span class="n">OmegaConf</span>

<span class="n">cfg</span> <span class="o">=</span> <span class="n">get_cfg</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">OPTIMIZER</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>NAME: Ranger
ARGUMENTS:
  betas:
  - 0.95
  - 0.999
  eps: 1.0e-05
  weight_decay: 0.01
  k: 6
  alpha: 0.5

</pre>
</div>
</div>

<div class="output_area">




<div id="f923f56c-4b70-486b-9d0e-be7ea8110293"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#f923f56c-4b70-486b-9d0e-be7ea8110293');

            setTimeout(function() {
                var nbb_cell_id = 17;
                var nbb_unformatted_code = "from lightning_cv.config import get_cfg\nfrom omegaconf import DictConfig, OmegaConf\n\ncfg = get_cfg()\nprint(OmegaConf.to_yaml(cfg.OPTIMIZER))";
                var nbb_formatted_code = "from lightning_cv.config import get_cfg\nfrom omegaconf import DictConfig, OmegaConf\n\ncfg = get_cfg()\nprint(OmegaConf.to_yaml(cfg.OPTIMIZER))";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p1</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-04</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-02</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">]</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">create_optimizer</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">cfg</span><span class="p">)</span>
<span class="c1"># opt</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="b813cec7-32d2-4541-96d3-8cd781d7b383"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#b813cec7-32d2-4541-96d3-8cd781d7b383');

            setTimeout(function() {
                var nbb_cell_id = 18;
                var nbb_unformatted_code = "# these also support multiple param groups\np1 = dict(params=[torch.nn.Parameter(torch.randn(1, 2))], lr=1e-04, weight_decay=0)\np2 = dict(params=[torch.nn.Parameter(torch.randn(2, 2))], lr=1e-02, weight_decay=0.1)\n\nparams = [p1, p2]\nopt = create_optimizer(params, cfg)\n# opt";
                var nbb_formatted_code = "# these also support multiple param groups\np1 = dict(params=[torch.nn.Parameter(torch.randn(1, 2))], lr=1e-04, weight_decay=0)\np2 = dict(params=[torch.nn.Parameter(torch.randn(2, 2))], lr=1e-02, weight_decay=0.1)\n\nparams = [p1, p2]\nopt = create_optimizer(params, cfg)\n# opt";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

