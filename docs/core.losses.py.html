---

title: Loss Functions


keywords: fastai
sidebar: home_sidebar

summary: "Custom LightningCV loss functions"
description: "Custom LightningCV loss functions"
nb_path: "nbs/00f_core.losses.py.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/00f_core.losses.py.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> nb_black
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="d16deeb9-4528-4610-9035-91fd0d497ecb"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#d16deeb9-4528-4610-9035-91fd0d497ecb');

            setTimeout(function() {
                var nbb_cell_id = 2;
                var nbb_unformatted_code = "%load_ext nb_black";
                var nbb_formatted_code = "%load_ext nb_black";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LabelSmoothingCrossEntropy" class="doc_header"><code>class</code> <code>LabelSmoothingCrossEntropy</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/losses.py#L20" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LabelSmoothingCrossEntropy</code>(<strong><code>eps</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>reduction</code></strong>:<code>str</code>=<em><code>'mean'</code></em>, <strong><code>weight</code></strong>:<code>Optional</code>[<code>Tensor</code>]=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Cross Entropy Loss with Label Smoothing</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="d294ba88-111d-4ad8-8c9a-296a9857449c"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#d294ba88-111d-4ad8-8c9a-296a9857449c');

            setTimeout(function() {
                var nbb_cell_id = 8;
                var nbb_unformatted_code = "# export\nclass LabelSmoothingCrossEntropy(nn.Module):\n    \"Cross Entropy Loss with Label Smoothing\"\n\n    def __init__(\n        self, eps: float = 0.1, reduction: str = \"mean\", weight: Optional[Tensor] = None\n    ):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        store_attr(\"eps, reduction, weight\")\n\n    def forward(self, input: Tensor, target: Tensor):\n        c = input.size()[1]\n        log_preds = F.log_softmax(input, dim=1)\n        if self.reduction == \"sum\":\n            loss = -log_preds.sum()\n        else:\n            loss = -log_preds.sum(dim=1)\n            if self.reduction == \"mean\":\n                loss = loss.mean()\n        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(\n            log_preds, target.long(), weight=self.weight, reduction=self.reduction\n        )\n        return loss";
                var nbb_formatted_code = "# export\nclass LabelSmoothingCrossEntropy(nn.Module):\n    \"Cross Entropy Loss with Label Smoothing\"\n\n    def __init__(\n        self, eps: float = 0.1, reduction: str = \"mean\", weight: Optional[Tensor] = None\n    ):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        store_attr(\"eps, reduction, weight\")\n\n    def forward(self, input: Tensor, target: Tensor):\n        c = input.size()[1]\n        log_preds = F.log_softmax(input, dim=1)\n        if self.reduction == \"sum\":\n            loss = -log_preds.sum()\n        else:\n            loss = -log_preds.sum(dim=1)\n            if self.reduction == \"mean\":\n                loss = loss.mean()\n        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(\n            log_preds, target.long(), weight=self.weight, reduction=self.reduction\n        )\n        return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Shape:</p>
<ul>
<li><code>input</code> (Tensor) : (N,C) where N is the mini-batch size and C is the total number of classes</li>
<li><code>target</code> (Tensor) : (N) where each value is $0 \leq \text{targets}[i] \leq C-10≤targets[i]≤C−1$</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lmce</span> <span class="o">=</span> <span class="n">LabelSmoothingCrossEntropy</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">lmce</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">[])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="015ec849-a310-4dc2-8d1f-dbed66705b98"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#015ec849-a310-4dc2-8d1f-dbed66705b98');

            setTimeout(function() {
                var nbb_cell_id = 9;
                var nbb_unformatted_code = "lmce = LabelSmoothingCrossEntropy(reduction=\"mean\")\ninput = torch.randn(32, 5, requires_grad=True)\ntarget = torch.empty(32, dtype=torch.long).random_(5)\nloss = lmce(input, target)\ntest_eq(loss.shape, [])";
                var nbb_formatted_code = "lmce = LabelSmoothingCrossEntropy(reduction=\"mean\")\ninput = torch.randn(32, 5, requires_grad=True)\ntarget = torch.empty(32, dtype=torch.long).random_(5)\nloss = lmce(input, target)\ntest_eq(loss.shape, [])";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="FocalLoss" class="doc_header"><code>class</code> <code>FocalLoss</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/losses.py#L44" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>FocalLoss</code>(<strong>*<code>args</code></strong>, <strong><code>gamma</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>2</code></em>, <strong><code>weight</code></strong>=<em><code>None</code></em>, <strong><code>ignore_index</code></strong>=<em><code>-100</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>, <strong>**<code>kwargs</code></strong>) :: <code>CrossEntropyLoss</code></p>
</blockquote>
<p>Same as <code>nn.CrossEntropyLoss</code> but with focal paramter, gamma. Focal loss is introduced by Lin et al.
<a href="https://arxiv.org/pdf/1708.02002.pdf">https://arxiv.org/pdf/1708.02002.pdf</a>.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="85416307-f3b0-448c-b19c-375d79bde61f"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#85416307-f3b0-448c-b19c-375d79bde61f');

            setTimeout(function() {
                var nbb_cell_id = 10;
                var nbb_unformatted_code = "# export\nclass FocalLoss(nn.CrossEntropyLoss):\n    \"\"\"\n    Same as `nn.CrossEntropyLoss` but with focal paramter, gamma. Focal loss is introduced by Lin et al.\n    https://arxiv.org/pdf/1708.02002.pdf.\n    \"\"\"\n\n    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction=\"mean\")\n    def __init__(self, *args, gamma: Union[float, int] = 2, **kwargs):\n        super(FocalLoss, self).__init__(**kwargs)\n        self.gamma = gamma\n        self.reduce = kwargs.pop(\"reduction\") if \"reduction\" in kwargs else \"mean\"\n        super().__init__(*args, reduction=\"none\", **kwargs)\n\n    def forward(self, input: Tensor, target: Tensor):\n        ce_loss = super().forward(input, target)\n        pt = torch.exp(-ce_loss)\n        fl_loss = (1 - pt) ** self.gamma * ce_loss\n        return (\n            fl_loss.mean()\n            if self.reduce == \"mean\"\n            else fl_loss.sum()\n            if self.reduce == \"sum\"\n            else fl_loss\n        )";
                var nbb_formatted_code = "# export\nclass FocalLoss(nn.CrossEntropyLoss):\n    \"\"\"\n    Same as `nn.CrossEntropyLoss` but with focal paramter, gamma. Focal loss is introduced by Lin et al.\n    https://arxiv.org/pdf/1708.02002.pdf.\n    \"\"\"\n\n    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction=\"mean\")\n    def __init__(self, *args, gamma: Union[float, int] = 2, **kwargs):\n        super(FocalLoss, self).__init__(**kwargs)\n        self.gamma = gamma\n        self.reduce = kwargs.pop(\"reduction\") if \"reduction\" in kwargs else \"mean\"\n        super().__init__(*args, reduction=\"none\", **kwargs)\n\n    def forward(self, input: Tensor, target: Tensor):\n        ce_loss = super().forward(input, target)\n        pt = torch.exp(-ce_loss)\n        fl_loss = (1 - pt) ** self.gamma * ce_loss\n        return (\n            fl_loss.mean()\n            if self.reduce == \"mean\"\n            else fl_loss.sum()\n            if self.reduce == \"sum\"\n            else fl_loss\n        )";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Shape:</p>
<ul>
<li><code>input</code> (Tensor) : (N,C) where N is the mini-batch size and C is the total number of classes</li>
<li><code>target</code> (Tensor) : (N) where each value is $0 \leq \text{targets}[i] \leq C-10≤targets[i]≤C−1$</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fn</span> <span class="o">=</span> <span class="n">FocalLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">[])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="70b50490-5831-4b5a-9345-f98ac1f52f32"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#70b50490-5831-4b5a-9345-f98ac1f52f32');

            setTimeout(function() {
                var nbb_cell_id = 12;
                var nbb_unformatted_code = "fn = FocalLoss(reduction=\"mean\")\ninput = torch.randn(32, 5, requires_grad=True)\ntarget = torch.empty(32, dtype=torch.long).random_(5)\nloss = fn(input, target)\ntest_eq(loss.shape, [])";
                var nbb_formatted_code = "fn = FocalLoss(reduction=\"mean\")\ninput = torch.randn(32, 5, requires_grad=True)\ntarget = torch.empty(32, dtype=torch.long).random_(5)\nloss = fn(input, target)\ntest_eq(loss.shape, [])";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SigmoidFocalLoss" class="doc_header"><code>class</code> <code>SigmoidFocalLoss</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/losses.py#L70" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SigmoidFocalLoss</code>(<strong><code>alpha</code></strong>:<code>float</code>=<em><code>-1</code></em>, <strong><code>gamma</code></strong>:<code>float</code>=<em><code>2</code></em>, <strong><code>reduction</code></strong>:<code>str</code>=<em><code>'mean'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Focal Loss with Sigmoid Activation used in RetinaNet for dense detection: <a href="https://arxiv.org/abs/1708.02002">https://arxiv.org/abs/1708.02002</a>.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="35ce9f03-301c-438c-b919-f4860d4528f0"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#35ce9f03-301c-438c-b919-f4860d4528f0');

            setTimeout(function() {
                var nbb_cell_id = 13;
                var nbb_unformatted_code = "# export\nclass SigmoidFocalLoss(nn.Module):\n    \"Focal Loss with Sigmoid Activation used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\"\n\n    def __init__(\n        self,\n        alpha: float = -1,\n        gamma: float = 2,\n        reduction: str = \"mean\",\n    ):\n        super(SigmoidFocalLoss, self).__init__()\n        store_attr(\"alpha, gamma, reduction\")\n\n    def forward(self, input: Tensor, target: Tensor):\n        target = maybe_convert_to_onehot(input, target)\n        loss = sigmoid_focal_loss(\n            input, target, self.gamma, self.alpha, self.reduction\n        )\n        return loss";
                var nbb_formatted_code = "# export\nclass SigmoidFocalLoss(nn.Module):\n    \"Focal Loss with Sigmoid Activation used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\"\n\n    def __init__(\n        self,\n        alpha: float = -1,\n        gamma: float = 2,\n        reduction: str = \"mean\",\n    ):\n        super(SigmoidFocalLoss, self).__init__()\n        store_attr(\"alpha, gamma, reduction\")\n\n    def forward(self, input: Tensor, target: Tensor):\n        target = maybe_convert_to_onehot(input, target)\n        loss = sigmoid_focal_loss(input, target, self.gamma, self.alpha, self.reduction)\n        return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Arguments to <a href="/lightning_cv/core.losses.py.html#SigmoidFocalLoss"><code>SigmoidFocalLoss</code></a>:</p>
<ul>
<li><code>alpha</code>: Weighting factor in <code>range (0,1)</code> to balance positive vs negative examples. <code>Default = -1 (no weighting)</code>. </li>
<li><code>gamma</code>: Exponent of the modulating factor <code>(1 - p_t)</code> to balance easy vs hard examples.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Shape:</p>
<ul>
<li><code>input</code> (Tensor) : (N,C) where N is the mini-batch size and C is the total number of classes</li>
<li><code>target</code> (Tensor) : Classification label for each element in input.</li>
</ul>
<p><code>target</code> can be :</p>
<ul>
<li>A long tensor of shape (N) where each value is $0 \leq \text{targets}[i] \leq C-10≤targets[i]≤C−1$</li>
<li>A float tensor with the same shape as input. Stores the binary classification label for each element in inputs <code>(0 for the negative class and 1 for the positive class)</code>.</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fn</span> <span class="o">=</span> <span class="n">SigmoidFocalLoss</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">one_hot_targets</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="n">loss1</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss2</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">one_hot_targets</span><span class="p">)</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">loss1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">[])</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">loss2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">[])</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">test_eq</span><span class="p">(</span><span class="n">loss1</span><span class="p">,</span> <span class="n">loss2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="fa2bcc44-c259-426f-b22b-569a31c25d0b"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#fa2bcc44-c259-426f-b22b-569a31c25d0b');

            setTimeout(function() {
                var nbb_cell_id = 14;
                var nbb_unformatted_code = "fn = SigmoidFocalLoss()\ninput = torch.randn(32, 5)\n\ntarget = torch.empty(32, dtype=torch.long).random_(5)\none_hot_targets = F.one_hot(target).float()\n\nloss1 = fn(input, target)\nloss2 = fn(input, one_hot_targets)\n\ntest_eq(loss1.shape, [])\ntest_eq(loss2.shape, [])\n\nwith torch.no_grad():\n    test_eq(loss1, loss2)";
                var nbb_formatted_code = "fn = SigmoidFocalLoss()\ninput = torch.randn(32, 5)\n\ntarget = torch.empty(32, dtype=torch.long).random_(5)\none_hot_targets = F.one_hot(target).float()\n\nloss1 = fn(input, target)\nloss2 = fn(input, one_hot_targets)\n\ntest_eq(loss1.shape, [])\ntest_eq(loss2.shape, [])\n\nwith torch.no_grad():\n    test_eq(loss1, loss2)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="log_t" class="doc_header"><code>log_t</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/losses.py#L88" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>log_t</code>(<strong><code>u</code></strong>, <strong><code>t</code></strong>)</p>
</blockquote>
<p>Compute log_t for <code>u</code>.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="a45a2de9-703c-43ce-a36a-9eb8fe0497a9"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#a45a2de9-703c-43ce-a36a-9eb8fe0497a9');

            setTimeout(function() {
                var nbb_cell_id = 16;
                var nbb_unformatted_code = "# export\ndef log_t(u, t):\n    \"\"\"Compute log_t for `u`.\"\"\"\n\n    if t == 1.0:\n        return torch.log(u)\n    else:\n        return (u ** (1.0 - t) - 1.0) / (1.0 - t)";
                var nbb_formatted_code = "# export\ndef log_t(u, t):\n    \"\"\"Compute log_t for `u`.\"\"\"\n\n    if t == 1.0:\n        return torch.log(u)\n    else:\n        return (u ** (1.0 - t) - 1.0) / (1.0 - t)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="exp_t" class="doc_header"><code>exp_t</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/losses.py#L97" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>exp_t</code>(<strong><code>u</code></strong>, <strong><code>t</code></strong>)</p>
</blockquote>
<p>Compute exp_t for <code>u</code>.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="d6f04904-f513-442d-94ca-61ed99e6d1a9"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#d6f04904-f513-442d-94ca-61ed99e6d1a9');

            setTimeout(function() {
                var nbb_cell_id = 17;
                var nbb_unformatted_code = "# export\ndef exp_t(u, t):\n    \"\"\"Compute exp_t for `u`.\"\"\"\n\n    if t == 1.0:\n        return torch.exp(u)\n    else:\n        return torch.relu(1.0 + (1.0 - t) * u) ** (1.0 / (1.0 - t))";
                var nbb_formatted_code = "# export\ndef exp_t(u, t):\n    \"\"\"Compute exp_t for `u`.\"\"\"\n\n    if t == 1.0:\n        return torch.exp(u)\n    else:\n        return torch.relu(1.0 + (1.0 - t) * u) ** (1.0 / (1.0 - t))";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="compute_normalization_fixed_point" class="doc_header"><code>compute_normalization_fixed_point</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/losses.py#L106" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>compute_normalization_fixed_point</code>(<strong><code>activations</code></strong>, <strong><code>t</code></strong>, <strong><code>num_iters</code></strong>=<em><code>5</code></em>)</p>
</blockquote>
<p>Returns the normalization value for each example (t &gt; 1.0).
Args:
activations: A multi-dimensional tensor with last dimension <code>num_classes</code>.
t: Temperature 2 (&gt; 1.0 for tail heaviness).
num_iters: Number of iterations to run the method.
Return: A tensor of same rank as activation with the last dimension being 1.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="fdb574a4-db4c-4d96-a03b-9f18ac004bb0"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#fdb574a4-db4c-4d96-a03b-9f18ac004bb0');

            setTimeout(function() {
                var nbb_cell_id = 18;
                var nbb_unformatted_code = "# export\ndef compute_normalization_fixed_point(activations, t, num_iters=5):\n    \"\"\"Returns the normalization value for each example (t > 1.0).\n    Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    t: Temperature 2 (> 1.0 for tail heaviness).\n    num_iters: Number of iterations to run the method.\n    Return: A tensor of same rank as activation with the last dimension being 1.\n    \"\"\"\n\n    mu = torch.max(activations, dim=-1).values.view(-1, 1)\n    normalized_activations_step_0 = activations - mu\n\n    normalized_activations = normalized_activations_step_0\n    i = 0\n    while i < num_iters:\n        i += 1\n        logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\n        normalized_activations = normalized_activations_step_0 * (\n            logt_partition ** (1.0 - t)\n        )\n\n    logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\n\n    return -log_t(1.0 / logt_partition, t) + mu";
                var nbb_formatted_code = "# export\ndef compute_normalization_fixed_point(activations, t, num_iters=5):\n    \"\"\"Returns the normalization value for each example (t > 1.0).\n    Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    t: Temperature 2 (> 1.0 for tail heaviness).\n    num_iters: Number of iterations to run the method.\n    Return: A tensor of same rank as activation with the last dimension being 1.\n    \"\"\"\n\n    mu = torch.max(activations, dim=-1).values.view(-1, 1)\n    normalized_activations_step_0 = activations - mu\n\n    normalized_activations = normalized_activations_step_0\n    i = 0\n    while i < num_iters:\n        i += 1\n        logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\n        normalized_activations = normalized_activations_step_0 * (\n            logt_partition ** (1.0 - t)\n        )\n\n    logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\n\n    return -log_t(1.0 / logt_partition, t) + mu";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="compute_normalization" class="doc_header"><code>compute_normalization</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/losses.py#L132" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>compute_normalization</code>(<strong><code>activations</code></strong>, <strong><code>t</code></strong>, <strong><code>num_iters</code></strong>=<em><code>5</code></em>)</p>
</blockquote>
<p>Returns the normalization value for each example.
Args:
activations: A multi-dimensional tensor with last dimension <code>num_classes</code>.
t: Temperature 2 (&lt; 1.0 for finite support, &gt; 1.0 for tail heaviness).
num_iters: Number of iterations to run the method.
Return: A tensor of same rank as activation with the last dimension being 1.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="6d9b6fc7-73a1-46a7-b180-787ff3b64e17"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#6d9b6fc7-73a1-46a7-b180-787ff3b64e17');

            setTimeout(function() {
                var nbb_cell_id = 19;
                var nbb_unformatted_code = "# export\ndef compute_normalization(activations, t, num_iters=5):\n    \"\"\"Returns the normalization value for each example.\n    Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    t: Temperature 2 (< 1.0 for finite support, > 1.0 for tail heaviness).\n    num_iters: Number of iterations to run the method.\n    Return: A tensor of same rank as activation with the last dimension being 1.\n    \"\"\"\n\n    if t < 1.0:\n        # not implemented as these values do not occur in the authors experiments...\n        return None\n    else:\n        return compute_normalization_fixed_point(activations, t, num_iters)";
                var nbb_formatted_code = "# export\ndef compute_normalization(activations, t, num_iters=5):\n    \"\"\"Returns the normalization value for each example.\n    Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    t: Temperature 2 (< 1.0 for finite support, > 1.0 for tail heaviness).\n    num_iters: Number of iterations to run the method.\n    Return: A tensor of same rank as activation with the last dimension being 1.\n    \"\"\"\n\n    if t < 1.0:\n        # not implemented as these values do not occur in the authors experiments...\n        return None\n    else:\n        return compute_normalization_fixed_point(activations, t, num_iters)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="tempered_softmax" class="doc_header"><code>tempered_softmax</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/losses.py#L148" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>tempered_softmax</code>(<strong><code>activations</code></strong>, <strong><code>t</code></strong>, <strong><code>num_iters</code></strong>=<em><code>5</code></em>)</p>
</blockquote>
<p>Tempered softmax function.
Args:
activations: A multi-dimensional tensor with last dimension <code>num_classes</code>.
t: Temperature tensor &gt; 0.0.
num_iters: Number of iterations to run the method.
Returns:
A probabilities tensor.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="addbc1fc-542d-4bc9-81a0-e59d7151b60d"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#addbc1fc-542d-4bc9-81a0-e59d7151b60d');

            setTimeout(function() {
                var nbb_cell_id = 20;
                var nbb_unformatted_code = "# export\ndef tempered_softmax(activations, t, num_iters=5):\n    \"\"\"Tempered softmax function.\n    Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    t: Temperature tensor > 0.0.\n    num_iters: Number of iterations to run the method.\n    Returns:\n    A probabilities tensor.\n    \"\"\"\n\n    if t == 1.0:\n        normalization_constants = torch.log(torch.sum(torch.exp(activations), dim=-1))\n    else:\n        normalization_constants = compute_normalization(activations, t, num_iters)\n\n    return exp_t(activations - normalization_constants, t)";
                var nbb_formatted_code = "# export\ndef tempered_softmax(activations, t, num_iters=5):\n    \"\"\"Tempered softmax function.\n    Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    t: Temperature tensor > 0.0.\n    num_iters: Number of iterations to run the method.\n    Returns:\n    A probabilities tensor.\n    \"\"\"\n\n    if t == 1.0:\n        normalization_constants = torch.log(torch.sum(torch.exp(activations), dim=-1))\n    else:\n        normalization_constants = compute_normalization(activations, t, num_iters)\n\n    return exp_t(activations - normalization_constants, t)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="bi_tempered_logistic_loss" class="doc_header"><code>bi_tempered_logistic_loss</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/losses.py#L166" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>bi_tempered_logistic_loss</code>(<strong><code>activations</code></strong>, <strong><code>labels</code></strong>, <strong><code>t1</code></strong>, <strong><code>t2</code></strong>, <strong><code>label_smoothing</code></strong>=<em><code>0.0</code></em>, <strong><code>num_iters</code></strong>=<em><code>5</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>)</p>
</blockquote>
<p>Bi-Tempered Logistic Loss with custom gradient.
Args:
activations: A multi-dimensional tensor with last dimension <code>num_classes</code>.
labels: A tensor with shape and dtype as activations.
t1: Temperature 1 (&lt; 1.0 for boundedness).
t2: Temperature 2 (&gt; 1.0 for tail heaviness, &lt; 1.0 for finite support).
label_smoothing: Label smoothing parameter between [0, 1).
num_iters: Number of iterations to run the method.
Returns:
A loss tensor.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="e1cf2d4d-33ba-4ea7-a4a0-576466dd4461"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#e1cf2d4d-33ba-4ea7-a4a0-576466dd4461');

            setTimeout(function() {
                var nbb_cell_id = 21;
                var nbb_unformatted_code = "# export\ndef bi_tempered_logistic_loss(\n    activations, labels, t1, t2, label_smoothing=0.0, num_iters=5, reduction=\"mean\"\n):\n    \"\"\"Bi-Tempered Logistic Loss with custom gradient.\n    Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    labels: A tensor with shape and dtype as activations.\n    t1: Temperature 1 (< 1.0 for boundedness).\n    t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n    label_smoothing: Label smoothing parameter between [0, 1).\n    num_iters: Number of iterations to run the method.\n    Returns:\n    A loss tensor.\n    \"\"\"\n    if label_smoothing > 0.0:\n        num_classes = labels.shape[-1]\n        labels = (\n            1 - num_classes / (num_classes - 1) * label_smoothing\n        ) * labels + label_smoothing / (num_classes - 1)\n\n    probabilities = tempered_softmax(activations, t2, num_iters)\n\n    temp1 = (log_t(labels + 1e-10, t1) - log_t(probabilities, t1)) * labels\n    temp2 = (1 / (2 - t1)) * (\n        torch.pow(labels, 2 - t1) - torch.pow(probabilities, 2 - t1)\n    )\n    loss_values = temp1 - temp2\n\n    if reduction == \"none\":\n        return loss_values\n    if reduction == \"sum\":\n        return loss_values.sum()\n    if reduction == \"mean\":\n        return loss_values.mean()";
                var nbb_formatted_code = "# export\ndef bi_tempered_logistic_loss(\n    activations, labels, t1, t2, label_smoothing=0.0, num_iters=5, reduction=\"mean\"\n):\n    \"\"\"Bi-Tempered Logistic Loss with custom gradient.\n    Args:\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\n    labels: A tensor with shape and dtype as activations.\n    t1: Temperature 1 (< 1.0 for boundedness).\n    t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n    label_smoothing: Label smoothing parameter between [0, 1).\n    num_iters: Number of iterations to run the method.\n    Returns:\n    A loss tensor.\n    \"\"\"\n    if label_smoothing > 0.0:\n        num_classes = labels.shape[-1]\n        labels = (\n            1 - num_classes / (num_classes - 1) * label_smoothing\n        ) * labels + label_smoothing / (num_classes - 1)\n\n    probabilities = tempered_softmax(activations, t2, num_iters)\n\n    temp1 = (log_t(labels + 1e-10, t1) - log_t(probabilities, t1)) * labels\n    temp2 = (1 / (2 - t1)) * (\n        torch.pow(labels, 2 - t1) - torch.pow(probabilities, 2 - t1)\n    )\n    loss_values = temp1 - temp2\n\n    if reduction == \"none\":\n        return loss_values\n    if reduction == \"sum\":\n        return loss_values.sum()\n    if reduction == \"mean\":\n        return loss_values.mean()";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BiTemperedLogisticLoss" class="doc_header"><code>class</code> <code>BiTemperedLogisticLoss</code><a href="https://github.com/benihime91/lightning_cv/tree/master/lightning_cv/core/losses.py#L202" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BiTemperedLogisticLoss</code>(<strong><code>eps</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>t1</code></strong>=<em><code>0.6</code></em>, <strong><code>t2</code></strong>=<em><code>1.4</code></em>, <strong><code>num_iters</code></strong>=<em><code>5</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Implementation of <a href="https://arxiv.org/abs/1906.03361">Robust Bi-Tempered Logistic Loss Based on Bregman Divergences</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="7f04a07e-955b-478f-9cd0-80a52f954851"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#7f04a07e-955b-478f-9cd0-80a52f954851');

            setTimeout(function() {
                var nbb_cell_id = 22;
                var nbb_unformatted_code = "# export\nclass BiTemperedLogisticLoss(nn.Module):\n    \"Implementation of [Robust Bi-Tempered Logistic Loss Based on Bregman Divergences](https://arxiv.org/abs/1906.03361)\"\n\n    def __init__(self, eps: float = 0.1, t1=0.6, t2=1.4, num_iters=5, reduction=\"mean\"):\n        super(BiTemperedLogisticLoss, self).__init__()\n        self.eps = eps\n        self.t1, self.t2 = t1, t2\n        self.num_iters = num_iters\n        self.reduction = reduction\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor):\n        target = maybe_convert_to_onehot(input, target)\n        loss = bi_tempered_logistic_loss(\n            input, target, self.t1, self.t2, self.eps, self.num_iters, self.reduction\n        )\n        return loss";
                var nbb_formatted_code = "# export\nclass BiTemperedLogisticLoss(nn.Module):\n    \"Implementation of [Robust Bi-Tempered Logistic Loss Based on Bregman Divergences](https://arxiv.org/abs/1906.03361)\"\n\n    def __init__(self, eps: float = 0.1, t1=0.6, t2=1.4, num_iters=5, reduction=\"mean\"):\n        super(BiTemperedLogisticLoss, self).__init__()\n        self.eps = eps\n        self.t1, self.t2 = t1, t2\n        self.num_iters = num_iters\n        self.reduction = reduction\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor):\n        target = maybe_convert_to_onehot(input, target)\n        loss = bi_tempered_logistic_loss(\n            input, target, self.t1, self.t2, self.eps, self.num_iters, self.reduction\n        )\n        return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Arguments to <a href="/lightning_cv/core.losses.py.html#BiTemperedLogisticLoss"><code>BiTemperedLogisticLoss</code></a> :</p>
<ul>
<li><code>t1</code>: Temperature 1 (&lt; 1.0 for boundedness).</li>
<li><code>t2</code>: Temperature 2 (&gt; 1.0 for tail heaviness, &lt; 1.0 for finite support).</li>
<li><code>eps</code>: Label Smoothing Factor.</li>
<li><code>num_iters</code>: Number of iterations to run the method.</li>
</ul>
<p>Shape:</p>
<ul>
<li><code>input</code> (Tensor) : (N,C) where N is the mini-batch size and C is the total number of classes</li>
<li><code>target</code> (Tensor) : Classification label for each element in input.</li>
</ul>
<p><code>target</code> can be :</p>
<ul>
<li>A long tensor of shape (N) where each value is $0 \leq \text{targets}[i] \leq C-10≤targets[i]≤C−1$</li>
<li>A float tensor with the same shape as input. Stores the binary classification label for each element in inputs <code>(0 for the negative class and 1 for the positive class)</code>.</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fn</span> <span class="o">=</span> <span class="n">BiTemperedLogisticLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">one_hot_targets</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="n">loss1</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss2</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">one_hot_targets</span><span class="p">)</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">loss1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">[])</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">loss2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">[])</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">test_eq</span><span class="p">(</span><span class="n">loss1</span><span class="p">,</span> <span class="n">loss2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="99e33433-634e-49b1-a015-cc793314a047"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#99e33433-634e-49b1-a015-cc793314a047');

            setTimeout(function() {
                var nbb_cell_id = 24;
                var nbb_unformatted_code = "fn = BiTemperedLogisticLoss(reduction=\"mean\")\ninput = torch.randn(32, 5)\n\ntarget = torch.empty(32, dtype=torch.long).random_(5)\none_hot_targets = F.one_hot(target).float()\n\nloss1 = fn(input, target)\nloss2 = fn(input, one_hot_targets)\n\ntest_eq(loss1.shape, [])\ntest_eq(loss2.shape, [])\n\nwith torch.no_grad():\n    test_eq(loss1, loss2)";
                var nbb_formatted_code = "fn = BiTemperedLogisticLoss(reduction=\"mean\")\ninput = torch.randn(32, 5)\n\ntarget = torch.empty(32, dtype=torch.long).random_(5)\none_hot_targets = F.one_hot(target).float()\n\nloss1 = fn(input, target)\nloss2 = fn(input, one_hot_targets)\n\ntest_eq(loss1.shape, [])\ntest_eq(loss2.shape, [])\n\nwith torch.no_grad():\n    test_eq(loss1, loss2)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

