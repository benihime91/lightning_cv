# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00e_core.layers.ipynb (unless otherwise specified).

__all__ = ['Identity', 'AdaptiveConcatPool2d', 'NormType', 'BatchNorm', 'LinBnDrop', 'MishJitAutoFn', 'mish', 'Mish',
           'ActivationCatalog', 'num_features_model', 'requires_grad', 'init_default', 'cond_init', 'norm_types',
           'apply_leaf', 'apply_init', 'set_bn_eval', 'bn_types', 'trainable_params', 'params']

# Cell
import torch
from torch import nn
import torch.nn.functional as F
from torch.nn import Module
from torch.jit import script

from fastcore.all import delegates
from enum import Enum
from functools import partial

from .common import Registry

# Cell
class Identity(Module):
    "Do nothing at all"

    def __init__(self):
        super(Identity, self).__init__()

    def forward(self, x):
        return x

# Cell
class AdaptiveConcatPool2d(Module):
    """
    Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`.
    From : https://github.com/fastai/fastai/blob/master/fastai/layers.py
    """
    def __init__(self, size=None):
        super(AdaptiveConcatPool2d, self).__init__()
        self.size = size or 1
        self.ap = nn.AdaptiveAvgPool2d(self.size)
        self.mp = nn.AdaptiveMaxPool2d(self.size)

    def forward(self, x):
        return torch.cat([self.mp(x), self.ap(x)], 1)

# Cell
NormType = Enum('NormType', 'Batch BatchZero Weight Spectral Instance InstanceZero')

# Cell
def _get_norm(prefix, nf, ndim=2, zero=False, **kwargs):
    "Norm layer with `nf` features and `ndim` initialized depending on `norm_type`."
    assert 1 <= ndim <= 3
    bn = getattr(nn, f"{prefix}{ndim}d")(nf, **kwargs)
    if bn.affine:
        bn.bias.data.fill_(1e-3)
        bn.weight.data.fill_(0. if zero else 1.)
    return bn

# Cell
@delegates(nn.BatchNorm2d)
def BatchNorm(nf, ndim=2, norm_type=NormType.Batch, **kwargs):
    """
    BatchNorm layer with `nf` features and `ndim` initialized depending on `norm_type`.
    From : https://github.com/fastai/fastai/blob/master/fastai/layers.py
    """
    return _get_norm('BatchNorm', nf, ndim, zero=norm_type==NormType.BatchZero, **kwargs)

# Cell
class LinBnDrop(nn.Sequential):
    """
    Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers.
    From : https://github.com/fastai/fastai/blob/master/fastai/layers.py
    """

    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=False):
        layers = [BatchNorm(n_out if lin_first else n_in, ndim=1)] if bn else []

        if p != 0:
            layers.append(nn.Dropout(p))

        lin = [nn.Linear(n_in, n_out, bias=not bn)]

        if act is not None:
            lin.append(act)

        layers = lin+layers if lin_first else layers+lin

        super().__init__(*layers)

# Cell
# Mish Activation Funtion
# Souce code : https://github.com/fastai/fastai/blob/master/fastai/layers.py
@script
def _mish_jit_fwd(x):
    return x.mul(torch.tanh(F.softplus(x)))

@script
def _mish_jit_bwd(x, grad_output):
    x_sigmoid = torch.sigmoid(x)
    x_tanh_sp = F.softplus(x).tanh()
    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))

class MishJitAutoFn(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        ctx.save_for_backward(x)
        return _mish_jit_fwd(x)

    @staticmethod
    def backward(ctx, grad_output):
        x = ctx.saved_variables[0]
        return _mish_jit_bwd(x, grad_output)

def mish(x):
    return MishJitAutoFn.apply(x)

# Cell
class Mish(Module):
    "Mish Activation function"
    def __init__(self, inplace=True):
        # NOTE: inplace does nothing it is for compatibility with `timm`
        super(Mish, self).__init__()

    def forward(self, x):
        return MishJitAutoFn.apply(x)

# Cell
ActivationCatalog = Registry("ACTIVATIONS")
ActivationCatalog.register(Mish)
ActivationCatalog.register(torch.nn.LeakyReLU)
ActivationCatalog.register(torch.nn.ReLU)
ActivationCatalog.register(torch.nn.GELU)
ActivationCatalog.register(torch.nn.Sigmoid)
ActivationCatalog.register(torch.nn.SiLU)
ActivationCatalog.register(torch.nn.Tanh)
ActivationCatalog.register(torch.nn.LogSoftmax)
ActivationCatalog.register(torch.nn.Softmax)

# Cell
@torch.no_grad()
def num_features_model(m, ch_int: int = 3):
    "Return the number of output features for `m`."
    sz = 32
    while True:
        try:
            x = torch.zeros((8, ch_int, sz, sz))
            dummy_out = m.eval()(x)
            return dummy_out.shape[1]
        except Exception as e:
            sz *= 2
            if sz > 2048: raise e

# Cell
# hide
# Note : Functions are taken directly from : https://github.com/fastai/fastai/blob/master/fastai/torch_core.py

# Cell
def requires_grad(m):
    "Check if the first parameter of `m` requires grad or not"
    ps = list(m.parameters())
    return ps[0].requires_grad if len(ps) > 0 else False

# Cell
def init_default(m, func=nn.init.kaiming_normal_):
    "Initialize `m` weights with `func` and set `bias` to 0."
    if func:
        if hasattr(m, 'weight'): func(m.weight)
        if hasattr(m, 'bias') and hasattr(m.bias, 'data'): m.bias.data.fill_(0.)
    return m

# Cell
norm_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d, nn.LayerNorm,)

def cond_init(m, func):
    "Apply `init_default` to `m` unless it's a batchnorm module"
    if (not isinstance(m, norm_types)) and requires_grad(m): init_default(m, func)

# Cell
def apply_leaf(m, f):
    "Apply `f` to children of `m`."
    c = m.children()
    if isinstance(m, nn.Module): f(m)
    for l in c: apply_leaf(l,f)

# Cell
def apply_init(m, func=nn.init.kaiming_normal_):
    "Initialize all non-batchnorm layers of `m` with `func`."
    apply_leaf(m, partial(cond_init, func=func))

# Cell
bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)

def set_bn_eval(m: Module):
    "Set bn layers in eval mode for all recursive children of `m`."
    for l in m.children():
        if isinstance(l, bn_types) and not next(l.parameters()).requires_grad:
            l.eval()
            for param in l.parameters():
                param.requires_grad = False
        set_bn_eval(l)

# Cell
def trainable_params(m):
    "Return all trainable parameters of `m`"
    return [p for p in m.parameters() if p.requires_grad]

# Cell
def params(m):
    "Return all parameters of `m`"
    return [p for p in m.parameters()]