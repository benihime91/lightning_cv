# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00d_core.schedules.ipynb (unless otherwise specified).

__all__ = ['LRMultiplier', 'FlatCos', 'WarmupParamScheduler', 'WarmupCosineLR', 'WarmupLinearLR', 'WarmupConstantLR']

# Cell
from typing import *
import math
import torch

from fvcore.common.param_scheduler import *

# Cell
class LRMultiplier(torch.optim.lr_scheduler._LRScheduler):
    """
    A LRScheduler which uses fvcore  `ParamScheduler` to multiply the
    learning rate of each param in the optimizer.
    Every step, the learning rate of each parameter becomes its initial value
    multiplied by the output of the given `ParamScheduler`.
    The absolute learning rate value of each parameter can be different.
    This scheduler can be used as long as the relative scale among them do
    not change during training.
    """

    # NOTES: in the most general case, every LR can use its own scheduler.
    # Supporting this requires interaction with the optimizer when its parameter
    # group is initialized. For example, classyvision implements its own optimizer
    # that allows different schedulers for every parameter group.
    # To avoid this complexity, we use this class to support the most common cases
    # where the relative scale among all LRs stay unchanged during training.  In this
    # case we only need a total of one scheduler that defines the relative LR multiplier.

    def __init__(self, optimizer: torch.optim.Optimizer, multiplier: ParamScheduler, max_iter: int, last_iter: int = -1,):
        """
        Args:
            optimizer, last_iter: See ``torch.optim.lr_scheduler._LRScheduler``.
                ``last_iter`` is the same as ``last_epoch``.
            multiplier: a fvcore ParamScheduler that defines the multiplier on
                every LR of the optimizer
            max_iter: the total number of training iterations
        """
        if not isinstance(multiplier, ParamScheduler):
            raise ValueError(
                "_LRMultiplier(multiplier=) must be an instance of fvcore "
                f"ParamScheduler. Got {multiplier} instead."
            )
        self._multiplier = multiplier
        self._max_iter = max_iter
        super().__init__(optimizer, last_epoch=last_iter)

    def state_dict(self):
        # fvcore schedulers are stateless. Only keep pytorch scheduler states
        return {"base_lrs": self.base_lrs, "last_epoch": self.last_epoch}

    def get_lr(self) -> List[float]:
        multiplier = self._multiplier(self.last_epoch / self._max_iter)
        return [base_lr * multiplier for base_lr in self.base_lrs]

# Cell
def FlatCos(optimizer: torch.optim.Optimizer, pct_start: float, max_iters: int):
    """
    Schedule the LearningRate at flat `lr` for `pct_start` of `max_iters` before cosine annealing.
    Inspired From - https://docs.fast.ai/callback.schedule.html#Learner.fit_flat_cos
    """

    schedulers = [LinearParamScheduler(1, 1), CosineParamScheduler(1, 0)]
    # create CompositeParamScheduler Class
    sched = CompositeParamScheduler(schedulers, lengths=[pct_start, 1-pct_start],
                                    interval_scaling=['rescaled', 'rescaled'])

    # Wrap Param Scheduler under LRMultiplier class
    sched = LRMultiplier(optimizer, sched, max_iter=max_iters)
    return sched

# Cell
class WarmupParamScheduler(CompositeParamScheduler):
    """
    Add an initial warmup stage to another scheduler.
    From - https://github.com/facebookresearch/fvcore/blob/master/fvcore/common/param_scheduler.py
    """

    def __init__(self, scheduler: ParamScheduler, warmup_factor: float, warmup_length: float, warmup_method: str = "linear",):
        """
        Args:
            scheduler: warmup will be added at the beginning of this scheduler
            warmup_factor: the factor w.r.t the initial value of ``scheduler``, e.g. 0.001
            warmup_length: the relative length (in [0, 1]) of warmup steps w.r.t the entire
                training, e.g. 0.01
            warmup_method: one of "linear" or "constant"
        """
        end_value   = scheduler(warmup_length)  # the value to reach when warmup ends
        start_value = warmup_factor * scheduler(0.0)
        if warmup_method == "constant":
            warmup = ConstantParamScheduler(start_value)
        elif warmup_method == "linear":
            warmup = LinearParamScheduler(start_value, end_value)
        else:
            raise ValueError("Unknown warmup method: {}".format(warmup_method))
        super().__init__([warmup, scheduler], interval_scaling=["rescaled", "fixed"],
                         lengths=[warmup_length, 1 - warmup_length],)

# Cell
def WarmupCosineLR(optimizer: torch.optim.Optimizer, warmup_iters: int, max_iters: int):
    "Linearly increase `lr` for `warmup_iters` before cosine annealing. "
    sched = CosineParamScheduler(1, 0)
    sched = WarmupParamScheduler(sched, warmup_factor=(1.0 / 1000),
                                 warmup_length=warmup_iters/max_iters,
                                 warmup_method="linear")
    return LRMultiplier(optimizer, multiplier=sched, max_iter=max_iters)

# Cell
def WarmupLinearLR(optimizer: torch.optim.Optimizer, warmup_iters: int, max_iters: int):
    "Linearly increase `lr` for `warmup_iters` before linearly decreasing the `lr`"
    sched = LinearParamScheduler(1, 0)
    sched = WarmupParamScheduler(sched, warmup_factor=(1.0 / 1000),
                                 warmup_length=warmup_iters/max_iters,
                                 warmup_method="linear")
    return LRMultiplier(optimizer, multiplier=sched, max_iter=max_iters)

# Cell
def WarmupConstantLR(optimizer: torch.optim.Optimizer, warmup_iters: int, max_iters: int):
    "Linearly increase `lr` for `warmup_iters` after which keep `lr` at constant value"
    sched = LinearParamScheduler(1, 1)
    sched = WarmupParamScheduler(sched, warmup_factor=(1.0 / 1000),
                                 warmup_length=warmup_iters/max_iters,
                                 warmup_method="linear")
    return LRMultiplier(optimizer, multiplier=sched, max_iter=max_iters)