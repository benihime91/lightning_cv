# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01d_classification.modelling.classifiers.ipynb (unless otherwise specified).

__all__ = ['GeneralClassifier', 'FastaiClassifier', 'CLASSIFICATION_CLASSIFIER_REGISTRY', 'create_classifier_head']

# Cell
from typing import *
import torch
from torch import nn
import torch.nn.functional as F

from timm.models.layers import create_classifier

from omegaconf import DictConfig
from fastcore.all import delegates, L

from ...core.layers import *
from ...core import Registry, ACTIVATION_REGISTERY

# Cell
from fastcore.all import *

# Cell
class GeneralClassifier(nn.Module):
    "A classification head w/ configurable global pooling and dropout that is similar to the one created by the timm library"

    @delegates(create_classifier)
    def __init__(
        self, num_features: int, num_classes: int, drop_rate: float = 0.0, **kwargs
    ):
        super(GeneralClassifier, self).__init__()
        self.global_pool, self.fc = create_classifier(
            num_features, num_classes, **kwargs
        )
        self.drop_rate = drop_rate

    def forward(self, x):
        x = self.global_pool(x)
        if self.drop_rate:
            x = F.dropout(x, p=float(self.drop_rate), training=self.training)
        x = self.fc(x)
        return x

    @classmethod
    def from_config(cls, cfg: DictConfig):
        return cls(**cfg)

# Cell
class FastaiClassifier(nn.Sequential):
    "A classification head same as head generated from : https://github.com/fastai/fastai/blob/master/fastai/vision/learner.py#L76"

    def __init__(
        self,
        num_features: int,
        num_classes: int,
        lin_ftrs: Optional[List] = None,
        ps: float = 0.5,
        concat_pool: bool = True,
        first_bn: bool = True,
        bn_final: bool = False,
        lin_first: bool = False,
        act_layer: str = "ReLU",
    ):

        # if using concat_pool then mult the number of features
        if concat_pool:
            num_features *= 2

        # features of the linear layers
        lin_ftrs = (
            [num_features, 512, num_classes]
            if lin_ftrs is None
            else [num_features] + lin_ftrs + [num_classes]
        )

        bns = [first_bn] + [True] * len(lin_ftrs[1:])

        ps = L(ps)

        if len(ps) == 1:
            ps = [ps[0] / 2] * (len(lin_ftrs) - 2) + ps

        act_layer = [ACTIVATION_REGISTERY.get(act_layer)(inplace=True)]
        actns = act_layer * (len(lin_ftrs) - 2) + [None]
        pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)

        layers = [pool, nn.Flatten()]

        if lin_first:
            layers.append(nn.Dropout(ps.pop(0)))

        for ni, no, bn, p, actn in zip(lin_ftrs[:-1], lin_ftrs[1:], bns, ps, actns):
            layers += LinBnDrop(ni, no, bn=bn, p=p, act=actn, lin_first=lin_first)

        if lin_first:
            layers.append(nn.Linear(lin_ftrs[-2], num_classes))

        if bn_final:
            layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))

        super(FastaiClassifier, self).__init__(*layers)

    @classmethod
    def from_config(cls, cfg: DictConfig):
        return cls(**cfg)

# Cell
CLASSIFICATION_CLASSIFIER_REGISTRY = Registry("Classification Classifiers")

CLASSIFICATION_CLASSIFIER_REGISTRY.__doc__ = (
    "Registry of Classifiers for Image Classification"
)
CLASSIFICATION_CLASSIFIER_REGISTRY.register(FastaiClassifier)
CLASSIFICATION_CLASSIFIER_REGISTRY.register(GeneralClassifier)

# Cell
def create_classifier_head(cfg: DictConfig):
    "instante an obj from `CNN_CLASSIFIER_REGISTRY` registery using lightning_cv.config"
    classifier_cls = CLASSIFICATION_CLASSIFIER_REGISTRY.get(cfg.MODEL.CLASSIFIER.NAME)
    classifier = classifier_cls.from_config(cfg.MODEL.CLASSIFIER.ARGUMENTS)
    return classifier