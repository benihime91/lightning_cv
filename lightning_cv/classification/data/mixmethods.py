# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01e_classification.data.mixmethods.ipynb (unless otherwise specified).

__all__ = ['MixHandler', 'NoMix', 'Mixup', 'Cutmix']

# Cell
from typing import Union
import numpy as np
import torch
from torch import tensor
import torch.nn.functional as F
from torch.distributions import Beta
import abc

from fastcore.all import delegates, ifnone, store_attr

# Cell
class MixHandler(abc.ABC):
    "A handler class for implementing `MixUp` style scheduling"

    def __init__(self, alpha: Union[int, float] = 0.5, conf_prob: float = 1.0):
        store_attr("alpha, conf_prob")
        if self.alpha > 0.0:
            self.distrib = Beta(tensor(alpha), tensor(alpha))
        else:
            self.distrib = Beta(tensor(1.0), tensor(1.0))

        self._has_ended = False

        self.lam_a = 1
        self.lam_b = 1 - self.lam_a

        self.inputs = None
        self.targets = None
        self.mix_inputs = None
        self.mix_targets = None
        self.device = None

    def reset_lams(self):
        self.lam_a = 1
        self.lam_b = 1 - self.lam_a

    def stop(self):
        "stops the mixmethod from furthur activation"
        self._has_ended = True

    def __call__(self, inputs: torch.Tensor, targets: torch.Tensor, *args, **kwargs):
        self.device = ifnone(self.device, inputs.device)

        if self._has_ended:
            self.targets, self.mix_targets = targets, targets
            self.inputs, self.mix_inputs = inputs, inputs
            self.reset_lams()

        else:
            self.targets, self.inputs = targets, inputs
            self.mix_inputs, self.mix_targets = self.call(
                inputs=inputs, targets=targets, *args, **kwargs
            )
        return self.mix_inputs

    @abc.abstractmethod
    def call(self, inputs: torch.Tensor, targets: torch.Tensor, *args, **kwargs):
        "The main function call for MixUP (and variants) exist here !!!"
        raise NotImplementedError

    def compute_loss(self, outputs: torch.Tensor, criterion):
        "Calculates loss for the Mixmethod"
        loss_a = criterion(outputs, self.targets)
        loss_b = criterion(outputs, self.mix_targets)
        return torch.mean(loss_a * self.lam_a + loss_b * self.lam_b)

# Cell
class NoMix(MixHandler):
    "Default MixHandler that does not apply any mix to the inputs"

    def __init__(self):
        super(NoMix, self).__init__()

    def call(self, inputs, targets, *args, **kwargs):
        self.inputs, self.targets = inputs, targets
        return inputs, targets

# Cell
class Mixup(MixHandler):
    "Implementation of https://arxiv.org/abs/1710.09412"

    @delegates(MixHandler)
    def __ini__(self, **kwargs):
        super(Mixup, self).__init__(**kwargs)

    def call(self, inputs: torch.Tensor, targets: torch.Tensor, *args, **kwargs):
        "Applys Mixup to a batch of inputs given `conf_prob`"
        r = np.random.rand(1)
        bs, _, H, W = inputs.size()

        if r < self.conf_prob:
            rand_index = torch.randperm(bs, device=self.device)
            self.lam_a = self.distrib.sample((1,))
            self.lam_a = torch.tensor(self.lam_a, device=self.device)
            self.lam_b = 1 - self.lam_a
            mix_inputs = self.lam_a * inputs + (self.lam_b) * inputs[rand_index, :]
            targets, mix_targets = targets, targets[rand_index]
            return mix_inputs, mix_targets
        else:
            self.reset_lams()
            return inputs, targets

# Cell
class Cutmix(MixHandler):
    "Implementation of `https://arxiv.org/abs/1905.04899`"

    @delegates(MixHandler)
    def __init__(self, alpha: Union[float, int] = 1.0, **kwargs):
        super(Cutmix, self).__init__(alpha=alpha, **kwargs)

    def rand_bbox(self, W, H, lam):
        cut_rat = torch.sqrt(1.0 - lam)
        cut_w = torch.round(W * cut_rat).type(torch.long).to(self.device)
        cut_h = torch.round(H * cut_rat).type(torch.long).to(self.device)
        # uniform
        cx = torch.randint(0, W, (1,), device=self.device)
        cy = torch.randint(0, H, (1,), device=self.device)
        x1 = torch.clamp(cx - cut_w // 2, 0, W)
        y1 = torch.clamp(cy - cut_h // 2, 0, H)
        x2 = torch.clamp(cx + cut_w // 2, 0, W)
        y2 = torch.clamp(cy + cut_h // 2, 0, H)
        return x1, y1, x2, y2

    def call(self, inputs: torch.Tensor, targets: torch.Tensor, *args, **kwargs):
        "Applys Mixup to a batch of inputs given `conf_prob`"
        r = np.random.rand(1)
        bs, _, H, W = inputs.size()

        if r < self.conf_prob:
            rand_index = torch.randperm(bs, device=self.device)
            self.lam_a = self.distrib.sample((1,))
            self.lam_a = torch.tensor(self.lam_a, device=self.device)
            self.lam_b = 1 - self.lam_a
            x1, y1, x2, y2 = self.rand_bbox(W, H, self.lam_a)
            targets, mix_targets = targets, targets[rand_index]
            # avoid modifying in place
            mix_inputs = inputs.clone()
            mix_inputs[:, :, x1:x2, y1:y2] = inputs[rand_index, :, x1:x2, y1:y2]

            # adjust lambda to exactly match pixel ratio
            lam = (1 - ((x2 - x1) * (y2 - y1)) / float(W * H)).item()
            self.lam_a = self.lam_a * lam
            self.lam_b = 1 - self.lam_a

            return mix_inputs, mix_targets
        else:
            self.reset_lams()
            return inputs, targets