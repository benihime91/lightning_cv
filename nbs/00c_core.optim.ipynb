{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ayushman/Desktop/lightning_cv/nbs/data'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.export import *\n",
    "from nbdev.imports import Config as NbdevConfig\n",
    "\n",
    "nbdev_path = str(NbdevConfig().path(\"nbs_path\")/'data')\n",
    "nbdev_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "> Collection of usefull `Optimizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from typing import *\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "from timm.optim import Lookahead, RAdam, RMSpropTF\n",
    "from fastcore.all import delegates\n",
    "from lightning_cv.core.common import Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# for test\n",
    "from fastcore.all import *\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ranger**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(RAdam)\n",
    "def Ranger(params: Iterable, betas:Tuple[float, float]=(.95, 0.999), eps:float=1e-5, \n",
    "           k: int=6, alpha: float=0.5, **kwargs):\n",
    "    \"Convenience method for `Lookahead` with `RAdam`\"\n",
    "    return Lookahead(RAdam(params, betas=betas, eps=eps, **kwargs), alpha=alpha, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Ranger\" class=\"doc_header\"><code>Ranger</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Ranger</code>(**`params`**:`Iterable`\\[`T_co`\\], **`betas`**:`Tuple`\\[`float`, `float`\\]=*`(0.95, 0.999)`*, **`eps`**:`float`=*`1e-05`*, **`k`**:`int`=*`6`*, **`alpha`**:`float`=*`0.5`*, **`lr`**=*`0.001`*, **`weight_decay`**=*`0`*)\n",
       "\n",
       "Convenience method for `Lookahead` with `RAdam`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Ranger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ranger with Gradient Centralization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class RangerGC(Optimizer):\n",
    "    \"\"\"\n",
    "    Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.  \n",
    "    From - https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params:Iterable, lr:float=1e-3, alpha:float=0.5, k:int=6, N_sma_threshhold:int=5,\n",
    "                 betas:Tuple[float, float]=(.95, 0.999), eps:float=1e-5, weight_decay:Union[float, int]=0, \n",
    "                 use_gc:bool=True, gc_conv_only:bool=False):\n",
    "\n",
    "        # parameter checks\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        if not lr > 0:\n",
    "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
    "        if not eps > 0:\n",
    "            raise ValueError(f'Invalid eps: {eps}')\n",
    "\n",
    "        # prep defaults and init torch.optim base\n",
    "        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n",
    "                        N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        # adjustable threshold\n",
    "        self.N_sma_threshhold = N_sma_threshhold\n",
    "\n",
    "        # look ahead params\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "        # radam buffer for state\n",
    "        self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
    "\n",
    "        # gc on or off\n",
    "        self.use_gc = use_gc\n",
    "\n",
    "        # level of gradient centralization\n",
    "        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        print(\"set state called\")\n",
    "        super(Ranger, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        # Evaluate averages and grad, update param tensors\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'Ranger optimizer does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "\n",
    "                    # look ahead weight storage now in state dict\n",
    "                    state['slow_buffer'] = torch.empty_like(p.data)\n",
    "                    state['slow_buffer'].copy_(p.data)\n",
    "\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
    "                        p_data_fp32)\n",
    "\n",
    "                # begin computations\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                # GC operation for Conv layers and FC layers\n",
    "                if grad.dim() > self.gc_gradient_threshold:\n",
    "                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # compute variance mov avg\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                # compute mean moving avg\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
    "\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * \\\n",
    "                        state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    if N_sma > self.N_sma_threshhold:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (\n",
    "                            N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay']\n",
    "                                     * group['lr'], p_data_fp32)\n",
    "\n",
    "                # apply lr\n",
    "                if N_sma > self.N_sma_threshhold:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size *\n",
    "                                         group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "                # integrated look ahead...\n",
    "                # we do it at the param level instead of group level\n",
    "                if state['step'] % group['k'] == 0:\n",
    "                    # get access to slow param tensor\n",
    "                    slow_p = state['slow_buffer']\n",
    "                    # (fast weights - slow weights) * alpha\n",
    "                    slow_p.add_(self.alpha, p.data - slow_p)\n",
    "                    # copy interpolated weights to RAdam param tensor\n",
    "                    p.data.copy_(slow_p)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"RangerGC\" class=\"doc_header\"><code>class</code> <code>RangerGC</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>RangerGC</code>(**`params`**:`Iterable`\\[`T_co`\\], **`lr`**:`float`=*`0.001`*, **`alpha`**:`float`=*`0.5`*, **`k`**:`int`=*`6`*, **`N_sma_threshhold`**:`int`=*`5`*, **`betas`**:`Tuple`\\[`float`, `float`\\]=*`(0.95, 0.999)`*, **`eps`**:`float`=*`1e-05`*, **`weight_decay`**:`Union`\\[`float`, `int`\\]=*`0`*, **`use_gc`**:`bool`=*`True`*, **`gc_conv_only`**:`bool`=*`False`*) :: `Optimizer`\n",
       "\n",
       "Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.  \n",
       "From - https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(RangerGC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGDP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SGDP(Optimizer):\n",
    "    \"SGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py\"\n",
    "    \n",
    "    def __init__(self, params: Iterable, lr = required, momentum: Union[float, int] = 0, \n",
    "                 dampening: Union[float, int] = 0, weight_decay: Union[float, int] = 0, \n",
    "                 nesterov: bool = False, eps: float = 1e-8, delta: float = 0.1, \n",
    "                 wd_ratio: Union[float, int] = 0.1):\n",
    "        \n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening, weight_decay=weight_decay,\n",
    "                        nesterov=nesterov, eps=eps, delta=delta, wd_ratio=wd_ratio)\n",
    "        super(SGDP, self).__init__(params, defaults)\n",
    "\n",
    "    def _channel_view(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def _layer_view(self, x):\n",
    "        return x.view(1, -1)\n",
    "\n",
    "    def _cosine_similarity(self, x, y, eps, view_func):\n",
    "        x = view_func(x)\n",
    "        y = view_func(y)\n",
    "\n",
    "        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n",
    "\n",
    "    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n",
    "        wd = 1\n",
    "        expand_size = [-1] + [1] * (len(p.shape) - 1)\n",
    "        for view_func in [self._channel_view, self._layer_view]:\n",
    "\n",
    "            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n",
    "\n",
    "            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n",
    "                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n",
    "                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n",
    "                wd = wd_ratio\n",
    "\n",
    "                return perturb, wd\n",
    "\n",
    "        return perturb, wd\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['momentum'] = torch.zeros_like(p.data)\n",
    "\n",
    "                # SGD\n",
    "                buf = state['momentum']\n",
    "                buf.mul_(momentum).add_(grad, alpha=1 - dampening)\n",
    "                if nesterov:\n",
    "                    d_p = grad + momentum * buf\n",
    "                else:\n",
    "                    d_p = buf\n",
    "\n",
    "                # Projection\n",
    "                wd_ratio = 1\n",
    "                if len(p.shape) > 1:\n",
    "                    d_p, wd_ratio = self._projection(p, grad, d_p, group['delta'], group['wd_ratio'], group['eps'])\n",
    "\n",
    "                # Weight decay\n",
    "                if group['weight_decay'] > 0:\n",
    "                    p.data.mul_(1 - group['lr'] * group['weight_decay'] * wd_ratio / (1-momentum))\n",
    "\n",
    "                # Step\n",
    "                p.data.add_(d_p, alpha=-group['lr'])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"SGDP\" class=\"doc_header\"><code>class</code> <code>SGDP</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>SGDP</code>(**`params`**:`Iterable`\\[`T_co`\\], **`lr`**=*`<required parameter>`*, **`momentum`**:`Union`\\[`float`, `int`\\]=*`0`*, **`dampening`**:`Union`\\[`float`, `int`\\]=*`0`*, **`weight_decay`**:`Union`\\[`float`, `int`\\]=*`0`*, **`nesterov`**:`bool`=*`False`*, **`eps`**:`float`=*`1e-08`*, **`delta`**:`float`=*`0.1`*, **`wd_ratio`**:`Union`\\[`float`, `int`\\]=*`0.1`*) :: `Optimizer`\n",
       "\n",
       "SGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SGDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdamP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AdamP(Optimizer):\n",
    "    \"AdamP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py\"\n",
    "    \n",
    "    def __init__(self, params: Iterable, lr: Union[float, int] = 1e-3, betas: Tuple[float, float] = (0.9, 0.999), \n",
    "                 eps: float = 1e-8, weight_decay: Union[float, int] = 0, delta: float = 0.1, \n",
    "                 wd_ratio: float = 0.1, nesterov: bool = False):\n",
    "        \n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
    "                        delta=delta, wd_ratio=wd_ratio, nesterov=nesterov)\n",
    "        \n",
    "        super(AdamP, self).__init__(params, defaults)\n",
    "\n",
    "    def _channel_view(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def _layer_view(self, x):\n",
    "        return x.view(1, -1)\n",
    "\n",
    "    def _cosine_similarity(self, x, y, eps, view_func):\n",
    "        x = view_func(x)\n",
    "        y = view_func(y)\n",
    "\n",
    "        x_norm = x.norm(dim=1).add_(eps)\n",
    "        y_norm = y.norm(dim=1).add_(eps)\n",
    "        dot = (x * y).sum(dim=1)\n",
    "\n",
    "        return dot.abs() / x_norm / y_norm\n",
    "\n",
    "    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n",
    "        wd = 1\n",
    "        expand_size = [-1] + [1] * (len(p.shape) - 1)\n",
    "        for view_func in [self._channel_view, self._layer_view]:\n",
    "\n",
    "            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n",
    "\n",
    "            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n",
    "                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n",
    "                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n",
    "                wd = wd_ratio\n",
    "\n",
    "                return perturb, wd\n",
    "\n",
    "        return perturb, wd\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad.data\n",
    "                beta1, beta2 = group['betas']\n",
    "                nesterov = group['nesterov']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                # Adam\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "\n",
    "                state['step'] += 1\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "                step_size = group['lr'] / bias_correction1\n",
    "\n",
    "                if nesterov:\n",
    "                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\n",
    "                else:\n",
    "                    perturb = exp_avg / denom\n",
    "\n",
    "                # Projection\n",
    "                wd_ratio = 1\n",
    "                if len(p.shape) > 1:\n",
    "                    perturb, wd_ratio = self._projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])\n",
    "\n",
    "                # Weight decay\n",
    "                if group['weight_decay'] > 0:\n",
    "                    p.data.mul_(1 - group['lr'] * group['weight_decay'] * wd_ratio)\n",
    "\n",
    "                # Step\n",
    "                p.data.add_(-step_size, perturb)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"AdamP\" class=\"doc_header\"><code>class</code> <code>AdamP</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>AdamP</code>(**`params`**:`Iterable`\\[`T_co`\\], **`lr`**:`Union`\\[`float`, `int`\\]=*`0.001`*, **`betas`**:`Tuple`\\[`float`, `float`\\]=*`(0.9, 0.999)`*, **`eps`**:`float`=*`1e-08`*, **`weight_decay`**:`Union`\\[`float`, `int`\\]=*`0`*, **`delta`**:`float`=*`0.1`*, **`wd_ratio`**:`float`=*`0.1`*, **`nesterov`**:`bool`=*`False`*) :: `Optimizer`\n",
       "\n",
       "AdamP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(AdamP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RMSpropTF from `timm`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"RMSpropTF\" class=\"doc_header\"><code>class</code> <code>RMSpropTF</code><a href=\"timm/optim/rmsprop_tf.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>RMSpropTF</code>(**`params`**, **`lr`**=*`0.01`*, **`alpha`**=*`0.9`*, **`eps`**=*`1e-10`*, **`weight_decay`**=*`0`*, **`momentum`**=*`0.0`*, **`centered`**=*`False`*, **`decoupled_decay`**=*`False`*, **`lr_in_momentum`**=*`True`*) :: `Optimizer`\n",
       "\n",
       "Implements RMSprop algorithm (TensorFlow style epsilon)\n",
       "\n",
       "NOTE: This is a direct cut-and-paste of PyTorch RMSprop with eps applied before sqrt\n",
       "and a few other modifications to closer match Tensorflow for matching hyper-params.\n",
       "\n",
       "Noteworthy changes include:\n",
       "1. Epsilon applied inside square-root\n",
       "2. square_avg initialized to ones\n",
       "3. LR scaling of update accumulated in momentum buffer\n",
       "\n",
       "Proposed by G. Hinton in his\n",
       "`course <http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_.\n",
       "\n",
       "The centered version first appears in `Generating Sequences\n",
       "With Recurrent Neural Networks <https://arxiv.org/pdf/1308.0850v5.pdf>`_.\n",
       "\n",
       "Arguments:\n",
       "    params (iterable): iterable of parameters to optimize or dicts defining\n",
       "        parameter groups\n",
       "    lr (float, optional): learning rate (default: 1e-2)\n",
       "    momentum (float, optional): momentum factor (default: 0)\n",
       "    alpha (float, optional): smoothing (decay) constant (default: 0.9)\n",
       "    eps (float, optional): term added to the denominator to improve\n",
       "        numerical stability (default: 1e-10)\n",
       "    centered (bool, optional) : if ``True``, compute the centered RMSProp,\n",
       "        the gradient is normalized by an estimation of its variance\n",
       "    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
       "    decoupled_decay (bool, optional): decoupled weight decay as per https://arxiv.org/abs/1711.05101\n",
       "    lr_in_momentum (bool, optional): learning rate scaling is included in the momentum buffer\n",
       "        update as per defaults in Tensorflow"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(RMSpropTF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIM_REGISTERY\n",
    "> `Registery` of Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.optim import SGD, Adam, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "OPTIM_REGISTERY = Registry(\"OPTIMIZERS\")\n",
    "OPTIM_REGISTERY.register(SGD)\n",
    "OPTIM_REGISTERY.register(SGDP)\n",
    "OPTIM_REGISTERY.register(Adam)\n",
    "OPTIM_REGISTERY.register(AdamW)\n",
    "OPTIM_REGISTERY.register(AdamP)\n",
    "OPTIM_REGISTERY.register(Ranger)\n",
    "OPTIM_REGISTERY.register(RangerGC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Registry of OPTIMIZERS:\n",
       "╒══════════╤═════════════════════════════════════╕\n",
       "│ Names    │ Objects                             │\n",
       "╞══════════╪═════════════════════════════════════╡\n",
       "│ SGD      │ <class 'torch.optim.sgd.SGD'>       │\n",
       "├──────────┼─────────────────────────────────────┤\n",
       "│ SGDP     │ <class '__main__.SGDP'>             │\n",
       "├──────────┼─────────────────────────────────────┤\n",
       "│ Adam     │ <class 'torch.optim.adam.Adam'>     │\n",
       "├──────────┼─────────────────────────────────────┤\n",
       "│ AdamW    │ <class 'torch.optim.adamw.AdamW'>   │\n",
       "├──────────┼─────────────────────────────────────┤\n",
       "│ AdamP    │ <class '__main__.AdamP'>            │\n",
       "├──────────┼─────────────────────────────────────┤\n",
       "│ Ranger   │ <function Ranger at 0x7f972710f430> │\n",
       "├──────────┼─────────────────────────────────────┤\n",
       "│ RangerGC │ <class '__main__.RangerGC'>         │\n",
       "╘══════════╧═════════════════════════════════════╛"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide-input\n",
    "OPTIM_REGISTERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_optimizer(params: Iterable, cfg=None):\n",
    "    \"Instante an optimizer from `OPTIM_REGISTRY` given `params` with lightning_cv `cfg`\"\n",
    "    opt_cls = OPTIM_REGISTERY.get(cfg.OPTIMIZER.NAME)\n",
    "    opt = opt_cls(params=params, **cfg.OPTIMIZER.ARGUMENTS)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an Optimizer using `create_optimizer` from LightningCv config -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: Ranger\n",
      "ARGUMENTS:\n",
      "  betas:\n",
      "  - 0.95\n",
      "  - 0.999\n",
      "  eps: 1.0e-05\n",
      "  weight_decay: 0.01\n",
      "  k: 6\n",
      "  alpha: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightning_cv.config import get_cfg\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "cfg = get_cfg(strict=False)\n",
    "print(OmegaConf.to_yaml(cfg.OPTIMIZER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lookahead (\n",
       "Parameter Group 0\n",
       "    betas: [0.95, 0.999]\n",
       "    eps: 1e-05\n",
       "    lookahead_alpha: 0.5\n",
       "    lookahead_k: 6\n",
       "    lookahead_step: 0\n",
       "    lr: 0.0001\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 1\n",
       "    betas: [0.95, 0.999]\n",
       "    eps: 1e-05\n",
       "    lookahead_alpha: 0.5\n",
       "    lookahead_k: 6\n",
       "    lookahead_step: 0\n",
       "    lr: 0.01\n",
       "    weight_decay: 0.1\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these also support multiple param groups\n",
    "p1 = dict(params=[torch.nn.Parameter(torch.randn(1, 2))], lr=1e-04, weight_decay=0)\n",
    "p2 = dict(params=[torch.nn.Parameter(torch.randn(2, 2))], lr=1e-02, weight_decay=0.1)\n",
    "\n",
    "params = [p1, p2]\n",
    "opt = create_optimizer(params, cfg)\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 00a_core.common.ipynb.\n",
      "Converted 00b_core.data_utils.ipynb.\n",
      "Converted 00c_core.optim.ipynb.\n",
      "Converted 00d_core.schedules.ipynb.\n",
      "Converted 00e_core.layers.ipynb.\n",
      "Converted 01a_classification.data.transforms.ipynb.\n",
      "Converted 01b_classification.data.datasets.ipynb.\n",
      "Converted 01c_classification.modelling.body.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_cv",
   "language": "python",
   "name": "lightning_cv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
