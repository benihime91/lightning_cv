{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incomplete-biodiversity",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "> Custom LightningCV loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-korean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 82;\n",
       "                var nbb_unformatted_code = \"# default_exp core.losses\";\n",
       "                var nbb_formatted_code = \"# default_exp core.losses\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp core.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-numbers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 83;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-gazette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 84;\n",
       "                var nbb_unformatted_code = \"# hide\\nimport warnings\\nfrom nbdev.showdoc import *\\nfrom nbdev.export import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_formatted_code = \"# hide\\nimport warnings\\nfrom nbdev.showdoc import *\\nfrom nbdev.export import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "import warnings\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.export import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-purchase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 85;\n",
       "                var nbb_unformatted_code = \"# export\\nfrom typing import *\\n\\nimport torch\\nfrom torch import nn\\nfrom torch import Tensor\\nimport torch.nn.functional as F\\nfrom fvcore.nn import sigmoid_focal_loss\\nfrom fastcore.all import store_attr, use_kwargs_dict\";\n",
       "                var nbb_formatted_code = \"# export\\nfrom typing import *\\n\\nimport torch\\nfrom torch import nn\\nfrom torch import Tensor\\nimport torch.nn.functional as F\\nfrom fvcore.nn import sigmoid_focal_loss\\nfrom fastcore.all import store_attr, use_kwargs_dict\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from fvcore.nn import sigmoid_focal_loss\n",
    "from fastcore.all import store_attr, use_kwargs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-multiple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 86;\n",
       "                var nbb_unformatted_code = \"# hide\\nfrom fastcore.all import *\";\n",
       "                var nbb_formatted_code = \"# hide\\nfrom fastcore.all import *\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-kitty",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-exhaust",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 87;\n",
       "                var nbb_unformatted_code = \"# export\\n@torch.no_grad()\\ndef maybe_convert_to_onehot(activations: Tensor, labels: Tensor):\\n    \\\"converts `labels` to onehot Tensors\\\"\\n    if len(labels.shape) < len(activations.shape):\\n        onehot = torch.zeros_like(activations)\\n        onehot.scatter_(1, labels[..., None], 1)\\n        labels = onehot\\n    return labels\";\n",
       "                var nbb_formatted_code = \"# export\\n@torch.no_grad()\\ndef maybe_convert_to_onehot(activations: Tensor, labels: Tensor):\\n    \\\"converts `labels` to onehot Tensors\\\"\\n    if len(labels.shape) < len(activations.shape):\\n        onehot = torch.zeros_like(activations)\\n        onehot.scatter_(1, labels[..., None], 1)\\n        labels = onehot\\n    return labels\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@torch.no_grad()\n",
    "def maybe_convert_to_onehot(activations: Tensor, labels: Tensor):\n",
    "    \"converts `labels` to onehot Tensors\"\n",
    "    if len(labels.shape) < len(activations.shape):\n",
    "        onehot = torch.zeros_like(activations)\n",
    "        onehot.scatter_(1, labels[..., None], 1)\n",
    "        labels = onehot\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-fiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 88;\n",
       "                var nbb_unformatted_code = \"activations = torch.randn(32, 5, requires_grad=True)\\nlabels = torch.empty(32, dtype=torch.long).random_(5)\\nonehots = maybe_convert_to_onehot(activations, labels)\\ntest_eq(onehots.shape, activations.shape)\";\n",
       "                var nbb_formatted_code = \"activations = torch.randn(32, 5, requires_grad=True)\\nlabels = torch.empty(32, dtype=torch.long).random_(5)\\nonehots = maybe_convert_to_onehot(activations, labels)\\ntest_eq(onehots.shape, activations.shape)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "activations = torch.randn(32, 5, requires_grad=True)\n",
    "labels = torch.empty(32, dtype=torch.long).random_(5)\n",
    "onehots = maybe_convert_to_onehot(activations, labels)\n",
    "test_eq(onehots.shape, activations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-liquid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 89;\n",
       "                var nbb_unformatted_code = \"# export\\ndef log_t(u, t):\\n    \\\"\\\"\\\"Compute log_t for `u`.\\\"\\\"\\\"\\n\\n    if t == 1.0:\\n        return torch.log(u)\\n    else:\\n        return (u ** (1.0 - t) - 1.0) / (1.0 - t)\";\n",
       "                var nbb_formatted_code = \"# export\\ndef log_t(u, t):\\n    \\\"\\\"\\\"Compute log_t for `u`.\\\"\\\"\\\"\\n\\n    if t == 1.0:\\n        return torch.log(u)\\n    else:\\n        return (u ** (1.0 - t) - 1.0) / (1.0 - t)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def log_t(u, t):\n",
    "    \"\"\"Compute log_t for `u`.\"\"\"\n",
    "\n",
    "    if t == 1.0:\n",
    "        return torch.log(u)\n",
    "    else:\n",
    "        return (u ** (1.0 - t) - 1.0) / (1.0 - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-postage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 90;\n",
       "                var nbb_unformatted_code = \"# export\\ndef exp_t(u, t):\\n    \\\"\\\"\\\"Compute exp_t for `u`.\\\"\\\"\\\"\\n\\n    if t == 1.0:\\n        return torch.exp(u)\\n    else:\\n        return torch.relu(1.0 + (1.0 - t) * u) ** (1.0 / (1.0 - t))\";\n",
       "                var nbb_formatted_code = \"# export\\ndef exp_t(u, t):\\n    \\\"\\\"\\\"Compute exp_t for `u`.\\\"\\\"\\\"\\n\\n    if t == 1.0:\\n        return torch.exp(u)\\n    else:\\n        return torch.relu(1.0 + (1.0 - t) * u) ** (1.0 / (1.0 - t))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def exp_t(u, t):\n",
    "    \"\"\"Compute exp_t for `u`.\"\"\"\n",
    "\n",
    "    if t == 1.0:\n",
    "        return torch.exp(u)\n",
    "    else:\n",
    "        return torch.relu(1.0 + (1.0 - t) * u) ** (1.0 / (1.0 - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-perry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 91;\n",
       "                var nbb_unformatted_code = \"# export\\ndef compute_normalization_fixed_point(activations, t, num_iters=5):\\n    \\\"\\\"\\\"Returns the normalization value for each example (t > 1.0).\\n    Args:\\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\\n    t: Temperature 2 (> 1.0 for tail heaviness).\\n    num_iters: Number of iterations to run the method.\\n    Return: A tensor of same rank as activation with the last dimension being 1.\\n    \\\"\\\"\\\"\\n\\n    mu = torch.max(activations, dim=-1).values.view(-1, 1)\\n    normalized_activations_step_0 = activations - mu\\n\\n    normalized_activations = normalized_activations_step_0\\n    i = 0\\n    while i < num_iters:\\n        i += 1\\n        logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\\n        normalized_activations = normalized_activations_step_0 * (\\n            logt_partition ** (1.0 - t)\\n        )\\n\\n    logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\\n\\n    return -log_t(1.0 / logt_partition, t) + mu\";\n",
       "                var nbb_formatted_code = \"# export\\ndef compute_normalization_fixed_point(activations, t, num_iters=5):\\n    \\\"\\\"\\\"Returns the normalization value for each example (t > 1.0).\\n    Args:\\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\\n    t: Temperature 2 (> 1.0 for tail heaviness).\\n    num_iters: Number of iterations to run the method.\\n    Return: A tensor of same rank as activation with the last dimension being 1.\\n    \\\"\\\"\\\"\\n\\n    mu = torch.max(activations, dim=-1).values.view(-1, 1)\\n    normalized_activations_step_0 = activations - mu\\n\\n    normalized_activations = normalized_activations_step_0\\n    i = 0\\n    while i < num_iters:\\n        i += 1\\n        logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\\n        normalized_activations = normalized_activations_step_0 * (\\n            logt_partition ** (1.0 - t)\\n        )\\n\\n    logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\\n\\n    return -log_t(1.0 / logt_partition, t) + mu\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def compute_normalization_fixed_point(activations, t, num_iters=5):\n",
    "    \"\"\"Returns the normalization value for each example (t > 1.0).\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature 2 (> 1.0 for tail heaviness).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "\n",
    "    mu = torch.max(activations, dim=-1).values.view(-1, 1)\n",
    "    normalized_activations_step_0 = activations - mu\n",
    "\n",
    "    normalized_activations = normalized_activations_step_0\n",
    "    i = 0\n",
    "    while i < num_iters:\n",
    "        i += 1\n",
    "        logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\n",
    "        normalized_activations = normalized_activations_step_0 * (\n",
    "            logt_partition ** (1.0 - t)\n",
    "        )\n",
    "\n",
    "    logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\n",
    "\n",
    "    return -log_t(1.0 / logt_partition, t) + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-patch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 92;\n",
       "                var nbb_unformatted_code = \"# export\\ndef compute_normalization(activations, t, num_iters=5):\\n    \\\"\\\"\\\"Returns the normalization value for each example.\\n    Args:\\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\\n    t: Temperature 2 (< 1.0 for finite support, > 1.0 for tail heaviness).\\n    num_iters: Number of iterations to run the method.\\n    Return: A tensor of same rank as activation with the last dimension being 1.\\n    \\\"\\\"\\\"\\n\\n    if t < 1.0:\\n        # not implemented as these values do not occur in the authors experiments...\\n        return None\\n    else:\\n        return compute_normalization_fixed_point(activations, t, num_iters)\";\n",
       "                var nbb_formatted_code = \"# export\\ndef compute_normalization(activations, t, num_iters=5):\\n    \\\"\\\"\\\"Returns the normalization value for each example.\\n    Args:\\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\\n    t: Temperature 2 (< 1.0 for finite support, > 1.0 for tail heaviness).\\n    num_iters: Number of iterations to run the method.\\n    Return: A tensor of same rank as activation with the last dimension being 1.\\n    \\\"\\\"\\\"\\n\\n    if t < 1.0:\\n        # not implemented as these values do not occur in the authors experiments...\\n        return None\\n    else:\\n        return compute_normalization_fixed_point(activations, t, num_iters)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def compute_normalization(activations, t, num_iters=5):\n",
    "    \"\"\"Returns the normalization value for each example.\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature 2 (< 1.0 for finite support, > 1.0 for tail heaviness).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "\n",
    "    if t < 1.0:\n",
    "        # not implemented as these values do not occur in the authors experiments...\n",
    "        return None\n",
    "    else:\n",
    "        return compute_normalization_fixed_point(activations, t, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-penguin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 93;\n",
       "                var nbb_unformatted_code = \"# export\\ndef tempered_softmax(activations, t, num_iters=5):\\n    \\\"\\\"\\\"Tempered softmax function.\\n    Args:\\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\\n    t: Temperature tensor > 0.0.\\n    num_iters: Number of iterations to run the method.\\n    Returns:\\n    A probabilities tensor.\\n    \\\"\\\"\\\"\\n\\n    if t == 1.0:\\n        normalization_constants = torch.log(torch.sum(torch.exp(activations), dim=-1))\\n    else:\\n        normalization_constants = compute_normalization(activations, t, num_iters)\\n\\n    return exp_t(activations - normalization_constants, t)\";\n",
       "                var nbb_formatted_code = \"# export\\ndef tempered_softmax(activations, t, num_iters=5):\\n    \\\"\\\"\\\"Tempered softmax function.\\n    Args:\\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\\n    t: Temperature tensor > 0.0.\\n    num_iters: Number of iterations to run the method.\\n    Returns:\\n    A probabilities tensor.\\n    \\\"\\\"\\\"\\n\\n    if t == 1.0:\\n        normalization_constants = torch.log(torch.sum(torch.exp(activations), dim=-1))\\n    else:\\n        normalization_constants = compute_normalization(activations, t, num_iters)\\n\\n    return exp_t(activations - normalization_constants, t)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def tempered_softmax(activations, t, num_iters=5):\n",
    "    \"\"\"Tempered softmax function.\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature tensor > 0.0.\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "    A probabilities tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    if t == 1.0:\n",
    "        normalization_constants = torch.log(torch.sum(torch.exp(activations), dim=-1))\n",
    "    else:\n",
    "        normalization_constants = compute_normalization(activations, t, num_iters)\n",
    "\n",
    "    return exp_t(activations - normalization_constants, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 94;\n",
       "                var nbb_unformatted_code = \"# export\\nclass LabelSmoothingCrossEntropy(nn.Module):\\n    \\\"Cross Entropy Loss with Label Smoothing\\\"\\n\\n    def __init__(\\n        self, eps: float = 0.1, reduction: str = \\\"mean\\\", weight: Optional[Tensor] = None\\n    ):\\n        super(LabelSmoothingCrossEntropy, self).__init__()\\n        store_attr(\\\"eps, reduction, weight\\\")\\n\\n    def forward(self, output: Tensor, target: Tensor):\\n        c = output.size()[1]\\n        log_preds = F.log_softmax(output, dim=1)\\n        if self.reduction == \\\"sum\\\":\\n            loss = -log_preds.sum()\\n        else:\\n            loss = -log_preds.sum(dim=1)\\n            if self.reduction == \\\"mean\\\":\\n                loss = loss.mean()\\n        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(\\n            log_preds, target.long(), weight=self.weight, reduction=self.reduction\\n        )\\n        return loss\";\n",
       "                var nbb_formatted_code = \"# export\\nclass LabelSmoothingCrossEntropy(nn.Module):\\n    \\\"Cross Entropy Loss with Label Smoothing\\\"\\n\\n    def __init__(\\n        self, eps: float = 0.1, reduction: str = \\\"mean\\\", weight: Optional[Tensor] = None\\n    ):\\n        super(LabelSmoothingCrossEntropy, self).__init__()\\n        store_attr(\\\"eps, reduction, weight\\\")\\n\\n    def forward(self, output: Tensor, target: Tensor):\\n        c = output.size()[1]\\n        log_preds = F.log_softmax(output, dim=1)\\n        if self.reduction == \\\"sum\\\":\\n            loss = -log_preds.sum()\\n        else:\\n            loss = -log_preds.sum(dim=1)\\n            if self.reduction == \\\"mean\\\":\\n                loss = loss.mean()\\n        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(\\n            log_preds, target.long(), weight=self.weight, reduction=self.reduction\\n        )\\n        return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"Cross Entropy Loss with Label Smoothing\"\n",
    "\n",
    "    def __init__(\n",
    "        self, eps: float = 0.1, reduction: str = \"mean\", weight: Optional[Tensor] = None\n",
    "    ):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        store_attr(\"eps, reduction, weight\")\n",
    "\n",
    "    def forward(self, output: Tensor, target: Tensor):\n",
    "        c = output.size()[1]\n",
    "        log_preds = F.log_softmax(output, dim=1)\n",
    "        if self.reduction == \"sum\":\n",
    "            loss = -log_preds.sum()\n",
    "        else:\n",
    "            loss = -log_preds.sum(dim=1)\n",
    "            if self.reduction == \"mean\":\n",
    "                loss = loss.mean()\n",
    "        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(\n",
    "            log_preds, target.long(), weight=self.weight, reduction=self.reduction\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-storage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 95;\n",
       "                var nbb_unformatted_code = \"lmce = LabelSmoothingCrossEntropy(reduction=\\\"mean\\\")\\noutput = torch.randn(32, 5, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(5)\\nloss = lmce(output, target)\\ntest_eq(loss.shape, [])\";\n",
       "                var nbb_formatted_code = \"lmce = LabelSmoothingCrossEntropy(reduction=\\\"mean\\\")\\noutput = torch.randn(32, 5, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(5)\\nloss = lmce(output, target)\\ntest_eq(loss.shape, [])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lmce = LabelSmoothingCrossEntropy(reduction=\"mean\")\n",
    "output = torch.randn(32, 5, requires_grad=True)\n",
    "target = torch.empty(32, dtype=torch.long).random_(5)\n",
    "loss = lmce(output, target)\n",
    "test_eq(loss.shape, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-nelson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 96;\n",
       "                var nbb_unformatted_code = \"# export\\nclass FocalLoss(nn.CrossEntropyLoss):\\n    \\\"\\\"\\\"\\n    Same as nn.CrossEntropyLoss but with focal paramter, gamma. Focal loss is introduced by Lin et al.\\n    https://arxiv.org/pdf/1708.02002.pdf.\\n    \\\"\\\"\\\"\\n\\n    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction=\\\"mean\\\")\\n    def __init__(self, *args, gamma: Union[float, int] = 2, **kwargs):\\n        super(FocalLoss, self).__init__(**kwargs)\\n        self.gamma = gamma\\n        self.reduce = kwargs.pop(\\\"reduction\\\") if \\\"reduction\\\" in kwargs else \\\"mean\\\"\\n        super().__init__(*args, reduction=\\\"none\\\", **kwargs)\\n\\n    def forward(self, output: Tensor, target: Tensor):\\n        ce_loss = super().forward(output, target)\\n        pt = torch.exp(-ce_loss)\\n        fl_loss = (1 - pt) ** self.gamma * ce_loss\\n        return (\\n            fl_loss.mean()\\n            if self.reduce == \\\"mean\\\"\\n            else fl_loss.sum()\\n            if self.reduce == \\\"sum\\\"\\n            else fl_loss\\n        )\";\n",
       "                var nbb_formatted_code = \"# export\\nclass FocalLoss(nn.CrossEntropyLoss):\\n    \\\"\\\"\\\"\\n    Same as nn.CrossEntropyLoss but with focal paramter, gamma. Focal loss is introduced by Lin et al.\\n    https://arxiv.org/pdf/1708.02002.pdf.\\n    \\\"\\\"\\\"\\n\\n    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction=\\\"mean\\\")\\n    def __init__(self, *args, gamma: Union[float, int] = 2, **kwargs):\\n        super(FocalLoss, self).__init__(**kwargs)\\n        self.gamma = gamma\\n        self.reduce = kwargs.pop(\\\"reduction\\\") if \\\"reduction\\\" in kwargs else \\\"mean\\\"\\n        super().__init__(*args, reduction=\\\"none\\\", **kwargs)\\n\\n    def forward(self, output: Tensor, target: Tensor):\\n        ce_loss = super().forward(output, target)\\n        pt = torch.exp(-ce_loss)\\n        fl_loss = (1 - pt) ** self.gamma * ce_loss\\n        return (\\n            fl_loss.mean()\\n            if self.reduce == \\\"mean\\\"\\n            else fl_loss.sum()\\n            if self.reduce == \\\"sum\\\"\\n            else fl_loss\\n        )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class FocalLoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"\n",
    "    Same as nn.CrossEntropyLoss but with focal paramter, gamma. Focal loss is introduced by Lin et al.\n",
    "    https://arxiv.org/pdf/1708.02002.pdf.\n",
    "    \"\"\"\n",
    "\n",
    "    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction=\"mean\")\n",
    "    def __init__(self, *args, gamma: Union[float, int] = 2, **kwargs):\n",
    "        super(FocalLoss, self).__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.reduce = kwargs.pop(\"reduction\") if \"reduction\" in kwargs else \"mean\"\n",
    "        super().__init__(*args, reduction=\"none\", **kwargs)\n",
    "\n",
    "    def forward(self, output: Tensor, target: Tensor):\n",
    "        ce_loss = super().forward(output, target)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        fl_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        return (\n",
    "            fl_loss.mean()\n",
    "            if self.reduce == \"mean\"\n",
    "            else fl_loss.sum()\n",
    "            if self.reduce == \"sum\"\n",
    "            else fl_loss\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-colombia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 97;\n",
       "                var nbb_unformatted_code = \"fn = FocalLoss(reduction=\\\"mean\\\")\\noutput = torch.randn(32, 5, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(5)\\nloss = fn(output, target)\\ntest_eq(loss.shape, [])\";\n",
       "                var nbb_formatted_code = \"fn = FocalLoss(reduction=\\\"mean\\\")\\noutput = torch.randn(32, 5, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(5)\\nloss = fn(output, target)\\ntest_eq(loss.shape, [])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fn = FocalLoss(reduction=\"mean\")\n",
    "output = torch.randn(32, 5, requires_grad=True)\n",
    "target = torch.empty(32, dtype=torch.long).random_(5)\n",
    "loss = fn(output, target)\n",
    "test_eq(loss.shape, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-bubble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 98;\n",
       "                var nbb_unformatted_code = \"# export\\nclass SigmoidFocalLoss(nn.Module):\\n    \\\"Focal Loss with Sigmoid Activation used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\\\"\\n\\n    def __init__(\\n        self,\\n        alpha: float = -1,\\n        gamma: float = 2,\\n        reduction: str = \\\"mean\\\",\\n    ):\\n        super(SigmoidFocalLoss, self).__init__()\\n        store_attr(\\\"alpha, gamma, reduction\\\")\\n\\n    def forward(self, output: Tensor, target: Tensor):\\n        target = maybe_convert_to_onehot(output, target)\\n        loss = sigmoid_focal_loss(\\n            output, target, self.gamma, self.alpha, self.reduction\\n        )\\n        return loss\";\n",
       "                var nbb_formatted_code = \"# export\\nclass SigmoidFocalLoss(nn.Module):\\n    \\\"Focal Loss with Sigmoid Activation used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\\\"\\n\\n    def __init__(\\n        self,\\n        alpha: float = -1,\\n        gamma: float = 2,\\n        reduction: str = \\\"mean\\\",\\n    ):\\n        super(SigmoidFocalLoss, self).__init__()\\n        store_attr(\\\"alpha, gamma, reduction\\\")\\n\\n    def forward(self, output: Tensor, target: Tensor):\\n        target = maybe_convert_to_onehot(output, target)\\n        loss = sigmoid_focal_loss(\\n            output, target, self.gamma, self.alpha, self.reduction\\n        )\\n        return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class SigmoidFocalLoss(nn.Module):\n",
    "    \"Focal Loss with Sigmoid Activation used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = -1,\n",
    "        gamma: float = 2,\n",
    "        reduction: str = \"mean\",\n",
    "    ):\n",
    "        super(SigmoidFocalLoss, self).__init__()\n",
    "        store_attr(\"alpha, gamma, reduction\")\n",
    "\n",
    "    def forward(self, output: Tensor, target: Tensor):\n",
    "        target = maybe_convert_to_onehot(output, target)\n",
    "        loss = sigmoid_focal_loss(\n",
    "            output, target, self.gamma, self.alpha, self.reduction\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-investing",
   "metadata": {},
   "source": [
    "Note,  \n",
    "`alpha` is the Weighting factor in range (0,1) to balance positive vs negative examples. Default = -1 (no weighting).  \n",
    "`gamma` is Exponent of the modulating factor `(1 - p_t)` to balance easy vs hard examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-brass",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 99;\n",
       "                var nbb_unformatted_code = \"fn = SigmoidFocalLoss()\\noutput = torch.randn(32, 5, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(5)\\nloss = fn(output, target)\\ntest_eq(loss.shape, [])\";\n",
       "                var nbb_formatted_code = \"fn = SigmoidFocalLoss()\\noutput = torch.randn(32, 5, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(5)\\nloss = fn(output, target)\\ntest_eq(loss.shape, [])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fn = SigmoidFocalLoss()\n",
    "output = torch.randn(32, 5, requires_grad=True)\n",
    "target = torch.empty(32, dtype=torch.long).random_(5)\n",
    "loss = fn(output, target)\n",
    "test_eq(loss.shape, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-serum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 100;\n",
       "                var nbb_unformatted_code = \"# export\\ndef bi_tempered_logistic_loss(\\n    activations, labels, t1, t2, label_smoothing=0.0, num_iters=5, reduction=\\\"mean\\\"\\n):\\n    \\\"\\\"\\\"Bi-Tempered Logistic Loss with custom gradient.\\n    Args:\\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\\n    labels: A tensor with shape and dtype as activations.\\n    t1: Temperature 1 (< 1.0 for boundedness).\\n    t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\\n    label_smoothing: Label smoothing parameter between [0, 1).\\n    num_iters: Number of iterations to run the method.\\n    Returns:\\n    A loss tensor.\\n    \\\"\\\"\\\"\\n    if label_smoothing > 0.0:\\n        num_classes = labels.shape[-1]\\n        labels = (\\n            1 - num_classes / (num_classes - 1) * label_smoothing\\n        ) * labels + label_smoothing / (num_classes - 1)\\n\\n    probabilities = tempered_softmax(activations, t2, num_iters)\\n\\n    temp1 = (log_t(labels + 1e-10, t1) - log_t(probabilities, t1)) * labels\\n    temp2 = (1 / (2 - t1)) * (\\n        torch.pow(labels, 2 - t1) - torch.pow(probabilities, 2 - t1)\\n    )\\n    loss_values = temp1 - temp2\\n\\n    if reduction == \\\"none\\\":\\n        return loss_values\\n    if reduction == \\\"sum\\\":\\n        return loss_values.sum()\\n    if reduction == \\\"mean\\\":\\n        return loss_values.mean()\";\n",
       "                var nbb_formatted_code = \"# export\\ndef bi_tempered_logistic_loss(\\n    activations, labels, t1, t2, label_smoothing=0.0, num_iters=5, reduction=\\\"mean\\\"\\n):\\n    \\\"\\\"\\\"Bi-Tempered Logistic Loss with custom gradient.\\n    Args:\\n    activations: A multi-dimensional tensor with last dimension `num_classes`.\\n    labels: A tensor with shape and dtype as activations.\\n    t1: Temperature 1 (< 1.0 for boundedness).\\n    t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\\n    label_smoothing: Label smoothing parameter between [0, 1).\\n    num_iters: Number of iterations to run the method.\\n    Returns:\\n    A loss tensor.\\n    \\\"\\\"\\\"\\n    if label_smoothing > 0.0:\\n        num_classes = labels.shape[-1]\\n        labels = (\\n            1 - num_classes / (num_classes - 1) * label_smoothing\\n        ) * labels + label_smoothing / (num_classes - 1)\\n\\n    probabilities = tempered_softmax(activations, t2, num_iters)\\n\\n    temp1 = (log_t(labels + 1e-10, t1) - log_t(probabilities, t1)) * labels\\n    temp2 = (1 / (2 - t1)) * (\\n        torch.pow(labels, 2 - t1) - torch.pow(probabilities, 2 - t1)\\n    )\\n    loss_values = temp1 - temp2\\n\\n    if reduction == \\\"none\\\":\\n        return loss_values\\n    if reduction == \\\"sum\\\":\\n        return loss_values.sum()\\n    if reduction == \\\"mean\\\":\\n        return loss_values.mean()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def bi_tempered_logistic_loss(\n",
    "    activations, labels, t1, t2, label_smoothing=0.0, num_iters=5, reduction=\"mean\"\n",
    "):\n",
    "    \"\"\"Bi-Tempered Logistic Loss with custom gradient.\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    labels: A tensor with shape and dtype as activations.\n",
    "    t1: Temperature 1 (< 1.0 for boundedness).\n",
    "    t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "    label_smoothing: Label smoothing parameter between [0, 1).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "    A loss tensor.\n",
    "    \"\"\"\n",
    "    if label_smoothing > 0.0:\n",
    "        num_classes = labels.shape[-1]\n",
    "        labels = (\n",
    "            1 - num_classes / (num_classes - 1) * label_smoothing\n",
    "        ) * labels + label_smoothing / (num_classes - 1)\n",
    "\n",
    "    probabilities = tempered_softmax(activations, t2, num_iters)\n",
    "\n",
    "    temp1 = (log_t(labels + 1e-10, t1) - log_t(probabilities, t1)) * labels\n",
    "    temp2 = (1 / (2 - t1)) * (\n",
    "        torch.pow(labels, 2 - t1) - torch.pow(probabilities, 2 - t1)\n",
    "    )\n",
    "    loss_values = temp1 - temp2\n",
    "\n",
    "    if reduction == \"none\":\n",
    "        return loss_values\n",
    "    if reduction == \"sum\":\n",
    "        return loss_values.sum()\n",
    "    if reduction == \"mean\":\n",
    "        return loss_values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-bryan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 101;\n",
       "                var nbb_unformatted_code = \"# export\\nclass BiTemperedLogisticLoss(nn.Module):\\n    \\\"Implementation of [Robust Bi-Tempered Logistic Loss Based on Bregman Divergences](https://arxiv.org/abs/1906.03361)\\\"\\n\\n    def __init__(self, eps: float = 0.1, t1=0.6, t2=1.4, num_iters=5, reduction=\\\"mean\\\"):\\n        super(BiTemperedLogisticLoss, self).__init__()\\n        self.eps = eps\\n        self.t1, self.t2 = t1, t2\\n        self.num_iters = num_iters\\n        self.reduction = reduction\\n\\n    def forward(self, output, target):\\n        target = maybe_convert_to_onehot(output, target)\\n        loss = bi_tempered_logistic_loss(\\n            output, target, self.t1, self.t2, self.eps, self.num_iters, self.reduction\\n        )\\n        return loss\";\n",
       "                var nbb_formatted_code = \"# export\\nclass BiTemperedLogisticLoss(nn.Module):\\n    \\\"Implementation of [Robust Bi-Tempered Logistic Loss Based on Bregman Divergences](https://arxiv.org/abs/1906.03361)\\\"\\n\\n    def __init__(self, eps: float = 0.1, t1=0.6, t2=1.4, num_iters=5, reduction=\\\"mean\\\"):\\n        super(BiTemperedLogisticLoss, self).__init__()\\n        self.eps = eps\\n        self.t1, self.t2 = t1, t2\\n        self.num_iters = num_iters\\n        self.reduction = reduction\\n\\n    def forward(self, output, target):\\n        target = maybe_convert_to_onehot(output, target)\\n        loss = bi_tempered_logistic_loss(\\n            output, target, self.t1, self.t2, self.eps, self.num_iters, self.reduction\\n        )\\n        return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class BiTemperedLogisticLoss(nn.Module):\n",
    "    \"Implementation of [Robust Bi-Tempered Logistic Loss Based on Bregman Divergences](https://arxiv.org/abs/1906.03361)\"\n",
    "\n",
    "    def __init__(self, eps: float = 0.1, t1=0.6, t2=1.4, num_iters=5, reduction=\"mean\"):\n",
    "        super(BiTemperedLogisticLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.t1, self.t2 = t1, t2\n",
    "        self.num_iters = num_iters\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        target = maybe_convert_to_onehot(output, target)\n",
    "        loss = bi_tempered_logistic_loss(\n",
    "            output, target, self.t1, self.t2, self.eps, self.num_iters, self.reduction\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-phoenix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 102;\n",
       "                var nbb_unformatted_code = \"fn = BiTemperedLogisticLoss(reduction=\\\"mean\\\")\\noutput = torch.randn(32, 5, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(5)\\nloss = fn(output, target)\\ntest_eq(loss.shape, [])\";\n",
       "                var nbb_formatted_code = \"fn = BiTemperedLogisticLoss(reduction=\\\"mean\\\")\\noutput = torch.randn(32, 5, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(5)\\nloss = fn(output, target)\\ntest_eq(loss.shape, [])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fn = BiTemperedLogisticLoss(reduction=\"mean\")\n",
    "output = torch.randn(32, 5, requires_grad=True)\n",
    "target = torch.empty(32, dtype=torch.long).random_(5)\n",
    "loss = fn(output, target)\n",
    "test_eq(loss.shape, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-flavor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 00a_core.utils.common.ipynb.\n",
      "Converted 00b_core.utils.data.ipynb.\n",
      "Converted 00c_core.optim.ipynb.\n",
      "Converted 00d_core.schedules.ipynb.\n",
      "Converted 00e_core.layers.ipynb.\n",
      "Converted 00f_core.losses.py.ipynb.\n",
      "Converted 01a_classification.data.transforms.ipynb.\n",
      "Converted 01b_classification.data.datasets.ipynb.\n",
      "Converted 01c_classification.modelling.backbones.ipynb.\n",
      "Converted 01d_classification.modelling.classifiers.ipynb.\n",
      "Converted 01e_classification.modelPL.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 103;\n",
       "                var nbb_unformatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_formatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-alpha",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_cv",
   "language": "python",
   "name": "lightning_cv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
