{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ayushman/Desktop/lightning_cv/nbs/data'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.export import *\n",
    "from nbdev.imports import Config as NbdevConfig\n",
    "\n",
    "nbdev_path = str(NbdevConfig().path(\"nbs_path\")/'data')\n",
    "nbdev_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "> Custom layers and basic functions to grab them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from enum import Enum\n",
    "from fastcore.all import delegates\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module\n",
    "from torch.jit import script\n",
    "\n",
    "from lightning_cv.core.utils.common import Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic manipulations and resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Identity(Module):\n",
    "    \"Do nothing at all\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(Identity()(1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AdaptiveConcatPool2d(Module):\n",
    "    \"\"\"\n",
    "    Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`.  \n",
    "    From : https://github.com/fastai/fastai/blob/master/fastai/layers.py\n",
    "    \"\"\"\n",
    "    def __init__(self, size=None):\n",
    "        super(AdaptiveConcatPool2d, self).__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool2d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(self.size)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return torch.cat([self.mp(x), self.ap(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = AdaptiveConcatPool2d()\n",
    "x = torch.randn(10,5,4,4)\n",
    "test_eq(tst(x).shape, [10,10,1,1])\n",
    "max1 = torch.max(x,    dim=2, keepdim=True)[0]\n",
    "maxp = torch.max(max1, dim=3, keepdim=True)[0]\n",
    "test_eq(tst(x)[:,:5], maxp)\n",
    "test_eq(tst(x)[:,5:], x.mean(dim=[2,3], keepdim=True))\n",
    "tst = AdaptiveConcatPool2d(2)\n",
    "test_eq(tst(x).shape, [10,10,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNorm layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "NormType = Enum('NormType', 'Batch BatchZero Weight Spectral Instance InstanceZero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_norm(prefix, nf, ndim=2, zero=False, **kwargs):\n",
    "    \"Norm layer with `nf` features and `ndim` initialized depending on `norm_type`.\"\n",
    "    assert 1 <= ndim <= 3\n",
    "    bn = getattr(nn, f\"{prefix}{ndim}d\")(nf, **kwargs)\n",
    "    if bn.affine:\n",
    "        bn.bias.data.fill_(1e-3)\n",
    "        bn.weight.data.fill_(0. if zero else 1.)\n",
    "    return bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(nn.BatchNorm2d)\n",
    "def BatchNorm(nf, ndim=2, norm_type=NormType.Batch, **kwargs):\n",
    "    \"\"\"\n",
    "    BatchNorm layer with `nf` features and `ndim` initialized depending on `norm_type`.  \n",
    "    From : https://github.com/fastai/fastai/blob/master/fastai/layers.py\n",
    "    \"\"\"\n",
    "    return _get_norm('BatchNorm', nf, ndim, zero=norm_type==NormType.BatchZero, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tst = BatchNorm(15)\n",
    "    assert isinstance(tst, nn.BatchNorm2d)\n",
    "    test_eq(tst.weight, torch.ones(15))\n",
    "    tst = BatchNorm(15, norm_type=NormType.BatchZero)\n",
    "    test_eq(tst.weight, torch.zeros(15))\n",
    "    tst = BatchNorm(15, ndim=1)\n",
    "    assert isinstance(tst, nn.BatchNorm1d)\n",
    "    tst = BatchNorm(15, ndim=3)\n",
    "    assert isinstance(tst, nn.BatchNorm3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LinBnDrop(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers.\n",
    "    From : https://github.com/fastai/fastai/blob/master/fastai/layers.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=False):\n",
    "        layers = [BatchNorm(n_out if lin_first else n_in, ndim=1)] if bn else []\n",
    "        \n",
    "        if p != 0: \n",
    "            layers.append(nn.Dropout(p))\n",
    "        \n",
    "        lin = [nn.Linear(n_in, n_out, bias=not bn)]\n",
    "        \n",
    "        if act is not None: \n",
    "            lin.append(act)\n",
    "        \n",
    "        layers = lin+layers if lin_first else layers+lin\n",
    "        \n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tst = LinBnDrop(10, 20)\n",
    "    mods = list(tst.children())\n",
    "    test_eq(len(mods), 2)\n",
    "    assert isinstance(mods[0], nn.BatchNorm1d)\n",
    "    assert isinstance(mods[1], nn.Linear)\n",
    "\n",
    "    tst = LinBnDrop(10, 20, p=0.1)\n",
    "    mods = list(tst.children())\n",
    "    test_eq(len(mods), 3)\n",
    "    assert isinstance(mods[0], nn.BatchNorm1d)\n",
    "    assert isinstance(mods[1], nn.Dropout)\n",
    "    assert isinstance(mods[2], nn.Linear)\n",
    "\n",
    "    tst = LinBnDrop(10, 20, act=nn.ReLU(), lin_first=True)\n",
    "    mods = list(tst.children())\n",
    "    test_eq(len(mods), 3)\n",
    "    assert isinstance(mods[0], nn.Linear)\n",
    "    assert isinstance(mods[1], nn.ReLU)\n",
    "    assert isinstance(mods[2], nn.BatchNorm1d)\n",
    "\n",
    "    tst = LinBnDrop(10, 20, bn=False)\n",
    "    mods = list(tst.children())\n",
    "    test_eq(len(mods), 1)\n",
    "    assert isinstance(mods[0], nn.Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "# Mish Activation Funtion\n",
    "# Souce code : https://github.com/fastai/fastai/blob/master/fastai/layers.py\n",
    "@script\n",
    "def _mish_jit_fwd(x): \n",
    "    return x.mul(torch.tanh(F.softplus(x)))\n",
    "\n",
    "@script\n",
    "def _mish_jit_bwd(x, grad_output):\n",
    "    x_sigmoid = torch.sigmoid(x)\n",
    "    x_tanh_sp = F.softplus(x).tanh()\n",
    "    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n",
    "\n",
    "class MishJitAutoFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return _mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_variables[0]\n",
    "        return _mish_jit_bwd(x, grad_output)\n",
    "\n",
    "def mish(x): \n",
    "    return MishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Mish(Module):\n",
    "    \"Mish Activation function\"\n",
    "    def __init__(self, inplace=True):\n",
    "        # NOTE: inplace does nothing it is for compatibility with `timm`\n",
    "        super(Mish, self).__init__()\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return MishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registery of Common Activation Functions -\n",
    "\n",
    "\n",
    "\n",
    "To add a activation function to the registery simply do :\n",
    "\n",
    "\n",
    "```python\n",
    "SomeActivation() # activation function\n",
    "ACTIVATION_REGISTERY.register(SomeActivation)\n",
    "# accesss it via\n",
    "act_func = ACTIVATION_REGISTERY.get(\"SomeActivation\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "ACTIVATION_REGISTERY = Registry(\"ACTIVATIONS\")\n",
    "ACTIVATION_REGISTERY.__doc__ = \"Registery of Activation Functions\"\n",
    "ACTIVATION_REGISTERY.register(Mish)\n",
    "ACTIVATION_REGISTERY.register(torch.nn.LeakyReLU)\n",
    "ACTIVATION_REGISTERY.register(torch.nn.ReLU)\n",
    "ACTIVATION_REGISTERY.register(torch.nn.GELU)\n",
    "ACTIVATION_REGISTERY.register(torch.nn.Sigmoid)\n",
    "ACTIVATION_REGISTERY.register(torch.nn.SiLU)\n",
    "ACTIVATION_REGISTERY.register(torch.nn.Tanh)\n",
    "ACTIVATION_REGISTERY.register(torch.nn.LogSoftmax)\n",
    "ACTIVATION_REGISTERY.register(torch.nn.Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registry of ACTIVATIONS:\n",
      "╒════════════╤══════════════════════════════════════════════════╕\n",
      "│ Names      │ Objects                                          │\n",
      "╞════════════╪══════════════════════════════════════════════════╡\n",
      "│ Mish       │ <class '__main__.Mish'>                          │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ LeakyReLU  │ <class 'torch.nn.modules.activation.LeakyReLU'>  │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ ReLU       │ <class 'torch.nn.modules.activation.ReLU'>       │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ GELU       │ <class 'torch.nn.modules.activation.GELU'>       │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ Sigmoid    │ <class 'torch.nn.modules.activation.Sigmoid'>    │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ SiLU       │ <class 'torch.nn.modules.activation.SiLU'>       │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ Tanh       │ <class 'torch.nn.modules.activation.Tanh'>       │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ LogSoftmax │ <class 'torch.nn.modules.activation.LogSoftmax'> │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ Softmax    │ <class 'torch.nn.modules.activation.Softmax'>    │\n",
      "╘════════════╧══════════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "#hide-input\n",
    "print(ACTIVATION_REGISTERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.no_grad()\n",
    "def num_features_model(m, ch_int: int = 3):\n",
    "    \"Return the number of output features for `m`.\"\n",
    "    sz = 32\n",
    "    while True:\n",
    "        try:\n",
    "            x = torch.zeros((8, ch_int, sz, sz))\n",
    "            dummy_out = m.eval()(x)\n",
    "            return dummy_out.shape[1]\n",
    "        except Exception as e:\n",
    "            sz *= 2\n",
    "            if sz > 2048: raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sequential(nn.Conv2d(3,5,3), nn.Conv2d(5,11,3))\n",
    "test_eq(num_features_model(m, ch_int=3), 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "# Note : Functions are taken directly from : https://github.com/fastai/fastai/blob/master/fastai/torch_core.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def requires_grad(m):\n",
    "    \"Check if the first parameter of `m` requires grad or not\"\n",
    "    ps = list(m.parameters())\n",
    "    return ps[0].requires_grad if len(ps) > 0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.Linear(4,5)\n",
    "assert requires_grad(tst)\n",
    "for p in tst.parameters(): p.requires_grad_(False)\n",
    "assert not requires_grad(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_default(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize `m` weights with `func` and set `bias` to 0.\"\n",
    "    if func:\n",
    "        if hasattr(m, 'weight'): func(m.weight)\n",
    "        if hasattr(m, 'bias') and hasattr(m.bias, 'data'): m.bias.data.fill_(0.)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tst = nn.Linear(4,5)\n",
    "    tst.weight.data.uniform_(-1,1)\n",
    "    tst.bias.data.uniform_(-1,1)\n",
    "    tst = init_default(tst, func = lambda x: x.data.fill_(1.))\n",
    "    test_eq(tst.weight, torch.ones(5,4))\n",
    "    test_eq(tst.bias, torch.zeros(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "norm_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d, nn.LayerNorm,)\n",
    "\n",
    "def cond_init(m, func):\n",
    "    \"Apply `init_default` to `m` unless it's a batchnorm module\"\n",
    "    if (not isinstance(m, norm_types)) and requires_grad(m): init_default(m, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tst = nn.Linear(4,5)\n",
    "    tst.weight.data.uniform_(-1,1)\n",
    "    tst.bias.data.uniform_(-1,1)\n",
    "    cond_init(tst, func = lambda x: x.data.fill_(1.))\n",
    "    test_eq(tst.weight, torch.ones(5,4))\n",
    "    test_eq(tst.bias, torch.zeros(5))\n",
    "\n",
    "    tst = nn.BatchNorm2d(5)\n",
    "    init = [tst.weight.clone(), tst.bias.clone()]\n",
    "    cond_init(tst, func = lambda x: x.data.fill_(1.))\n",
    "    test_eq(tst.weight, init[0])\n",
    "    test_eq(tst.bias, init[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_leaf(m, f):\n",
    "    \"Apply `f` to children of `m`.\"\n",
    "    c = m.children()\n",
    "    if isinstance(m, nn.Module): f(m)\n",
    "    for l in c: apply_leaf(l,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tst = nn.Sequential(nn.Linear(4,5), nn.Sequential(nn.Linear(4,5), nn.Linear(4,5)))\n",
    "    apply_leaf(tst, partial(init_default, func=lambda x: x.data.fill_(1.)))\n",
    "    for l in [tst[0], *tst[1]]: test_eq(l.weight, torch.ones(5,4))\n",
    "    for l in [tst[0], *tst[1]]: test_eq(l.bias,   torch.zeros(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_init(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize all non-batchnorm layers of `m` with `func`.\"\n",
    "    apply_leaf(m, partial(cond_init, func=func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tst = nn.Sequential(nn.Linear(4,5), nn.Sequential(nn.Linear(4,5), nn.BatchNorm1d(5)))\n",
    "    init = [tst[1][1].weight.clone(), tst[1][1].bias.clone()]\n",
    "    apply_init(tst, func=lambda x: x.data.fill_(1.))\n",
    "    for l in [tst[0], tst[1][0]]: test_eq(l.weight, torch.ones(5,4))\n",
    "    for l in [tst[0], tst[1][0]]: test_eq(l.bias,   torch.zeros(5))\n",
    "    test_eq(tst[1][1].weight, init[0])\n",
    "    test_eq(tst[1][1].bias,   init[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n",
    "\n",
    "def set_bn_eval(m: Module):\n",
    "    \"Set bn layers in eval mode for all recursive children of `m`.\"\n",
    "    for l in m.children():\n",
    "        if isinstance(l, bn_types) and not next(l.parameters()).requires_grad:\n",
    "            l.eval()\n",
    "            for param in l.parameters(): \n",
    "                param.requires_grad = False\n",
    "        set_bn_eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def trainable_params(m):\n",
    "    \"Return all trainable parameters of `m`\"\n",
    "    return [p for p in m.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def params(m):\n",
    "    \"Return all parameters of `m`\"\n",
    "    return [p for p in m.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    m = nn.Linear(4,5)\n",
    "    test_eq(trainable_params(m), [m.weight, m.bias])\n",
    "    m.weight.requires_grad_(False)\n",
    "    test_eq(trainable_params(m), [m.bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 00a_core.common.ipynb.\n",
      "Converted 00b_core.data_utils.ipynb.\n",
      "Converted 00c_core.optim.ipynb.\n",
      "Converted 00d_core.schedules.ipynb.\n",
      "Converted 00e_core.layers.ipynb.\n",
      "Converted 01a_classification.data.transforms.ipynb.\n",
      "Converted 01b_classification.data.datasets.ipynb.\n",
      "Converted 01c_classification.modelling.body.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_cv",
   "language": "python",
   "name": "lightning_cv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
