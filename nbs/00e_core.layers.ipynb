{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ayushman/Desktop/lightning_cv/nbs/data'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.export import *\n",
    "from nbdev.imports import Config as NbdevConfig\n",
    "\n",
    "nbdev_path = str(NbdevConfig().path(\"nbs_path\")/'data')\n",
    "nbdev_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "> Custom layers and basic functions to grab them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module\n",
    "from torch.jit import script\n",
    "\n",
    "from fastcore.all import delegates\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "\n",
    "from lightning_cv.core.common import Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic manipulations and resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Identity(Module):\n",
    "    \"Do nothing at all\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(Identity()(1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AdaptiveConcatPool2d(Module):\n",
    "    \"\"\"\n",
    "    Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`.  \n",
    "    From : https://github.com/fastai/fastai/blob/master/fastai/layers.py\n",
    "    \"\"\"\n",
    "    def __init__(self, size=None):\n",
    "        super(AdaptiveConcatPool2d, self).__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool2d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(self.size)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        return torch.cat([self.mp(x), self.ap(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = AdaptiveConcatPool2d()\n",
    "x = torch.randn(10,5,4,4)\n",
    "test_eq(tst(x).shape, [10,10,1,1])\n",
    "max1 = torch.max(x,    dim=2, keepdim=True)[0]\n",
    "maxp = torch.max(max1, dim=3, keepdim=True)[0]\n",
    "test_eq(tst(x)[:,:5], maxp)\n",
    "test_eq(tst(x)[:,5:], x.mean(dim=[2,3], keepdim=True))\n",
    "tst = AdaptiveConcatPool2d(2)\n",
    "test_eq(tst(x).shape, [10,10,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNorm layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "NormType = Enum('NormType', 'Batch BatchZero Weight Spectral Instance InstanceZero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_norm(prefix, nf, ndim=2, zero=False, **kwargs):\n",
    "    \"Norm layer with `nf` features and `ndim` initialized depending on `norm_type`.\"\n",
    "    assert 1 <= ndim <= 3\n",
    "    bn = getattr(nn, f\"{prefix}{ndim}d\")(nf, **kwargs)\n",
    "    if bn.affine:\n",
    "        bn.bias.data.fill_(1e-3)\n",
    "        bn.weight.data.fill_(0. if zero else 1.)\n",
    "    return bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(nn.BatchNorm2d)\n",
    "def BatchNorm(nf, ndim=2, norm_type=NormType.Batch, **kwargs):\n",
    "    \"\"\"\n",
    "    BatchNorm layer with `nf` features and `ndim` initialized depending on `norm_type`.  \n",
    "    From : https://github.com/fastai/fastai/blob/master/fastai/layers.py\n",
    "    \"\"\"\n",
    "    return _get_norm('BatchNorm', nf, ndim, zero=norm_type==NormType.BatchZero, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tst = BatchNorm(15)\n",
    "    assert isinstance(tst, nn.BatchNorm2d)\n",
    "    test_eq(tst.weight, torch.ones(15))\n",
    "    tst = BatchNorm(15, norm_type=NormType.BatchZero)\n",
    "    test_eq(tst.weight, torch.zeros(15))\n",
    "    tst = BatchNorm(15, ndim=1)\n",
    "    assert isinstance(tst, nn.BatchNorm1d)\n",
    "    tst = BatchNorm(15, ndim=3)\n",
    "    assert isinstance(tst, nn.BatchNorm3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LinBnDrop(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers.\n",
    "    From : https://github.com/fastai/fastai/blob/master/fastai/layers.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=False):\n",
    "        layers = [BatchNorm(n_out if lin_first else n_in, ndim=1)] if bn else []\n",
    "        \n",
    "        if p != 0: \n",
    "            layers.append(nn.Dropout(p))\n",
    "        \n",
    "        lin = [nn.Linear(n_in, n_out, bias=not bn)]\n",
    "        \n",
    "        if act is not None: \n",
    "            lin.append(act)\n",
    "        \n",
    "        layers = lin+layers if lin_first else layers+lin\n",
    "        \n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tst = LinBnDrop(10, 20)\n",
    "    mods = list(tst.children())\n",
    "    test_eq(len(mods), 2)\n",
    "    assert isinstance(mods[0], nn.BatchNorm1d)\n",
    "    assert isinstance(mods[1], nn.Linear)\n",
    "\n",
    "    tst = LinBnDrop(10, 20, p=0.1)\n",
    "    mods = list(tst.children())\n",
    "    test_eq(len(mods), 3)\n",
    "    assert isinstance(mods[0], nn.BatchNorm1d)\n",
    "    assert isinstance(mods[1], nn.Dropout)\n",
    "    assert isinstance(mods[2], nn.Linear)\n",
    "\n",
    "    tst = LinBnDrop(10, 20, act=nn.ReLU(), lin_first=True)\n",
    "    mods = list(tst.children())\n",
    "    test_eq(len(mods), 3)\n",
    "    assert isinstance(mods[0], nn.Linear)\n",
    "    assert isinstance(mods[1], nn.ReLU)\n",
    "    assert isinstance(mods[2], nn.BatchNorm1d)\n",
    "\n",
    "    tst = LinBnDrop(10, 20, bn=False)\n",
    "    mods = list(tst.children())\n",
    "    test_eq(len(mods), 1)\n",
    "    assert isinstance(mods[0], nn.Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@script\n",
    "def _mish_jit_fwd(x): return x.mul(torch.tanh(F.softplus(x)))\n",
    "\n",
    "@script\n",
    "def _mish_jit_bwd(x, grad_output):\n",
    "    x_sigmoid = torch.sigmoid(x)\n",
    "    x_tanh_sp = F.softplus(x).tanh()\n",
    "    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n",
    "\n",
    "class MishJitAutoFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return _mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_variables[0]\n",
    "        return _mish_jit_bwd(x, grad_output)\n",
    "\n",
    "def mish(x): \n",
    "    return MishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Mish(Module):\n",
    "    \"Mish Activation function\"\n",
    "    def __init__(self, inplace=True):\n",
    "        # NOTE: inplace does nothing it is for compatibility with `timm`\n",
    "        super(Mish, self).__init__()\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return MishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registery of Activations -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "ActivationCatalog = Registry(\"ACTIVATIONS\")\n",
    "ActivationCatalog.register(Mish)\n",
    "ActivationCatalog.register(torch.nn.LeakyReLU)\n",
    "ActivationCatalog.register(torch.nn.ReLU)\n",
    "ActivationCatalog.register(torch.nn.GELU)\n",
    "ActivationCatalog.register(torch.nn.Sigmoid)\n",
    "ActivationCatalog.register(torch.nn.SiLU)\n",
    "ActivationCatalog.register(torch.nn.Tanh)\n",
    "ActivationCatalog.register(torch.nn.LogSoftmax)\n",
    "ActivationCatalog.register(torch.nn.Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registry of ACTIVATIONS:\n",
      "╒════════════╤══════════════════════════════════════════════════╕\n",
      "│ Names      │ Objects                                          │\n",
      "╞════════════╪══════════════════════════════════════════════════╡\n",
      "│ Mish       │ <class '__main__.Mish'>                          │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ LeakyReLU  │ <class 'torch.nn.modules.activation.LeakyReLU'>  │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ ReLU       │ <class 'torch.nn.modules.activation.ReLU'>       │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ GELU       │ <class 'torch.nn.modules.activation.GELU'>       │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ Sigmoid    │ <class 'torch.nn.modules.activation.Sigmoid'>    │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ SiLU       │ <class 'torch.nn.modules.activation.SiLU'>       │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ Tanh       │ <class 'torch.nn.modules.activation.Tanh'>       │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ LogSoftmax │ <class 'torch.nn.modules.activation.LogSoftmax'> │\n",
      "├────────────┼──────────────────────────────────────────────────┤\n",
      "│ Softmax    │ <class 'torch.nn.modules.activation.Softmax'>    │\n",
      "╘════════════╧══════════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "print(ActivationCatalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.no_grad()\n",
    "def num_features_model(m, ch_int: int = 3):\n",
    "    \"Return the number of output features for `m`.\"\n",
    "    sz = 32\n",
    "    while True:\n",
    "        try:\n",
    "            x = torch.zeros((8, ch_int, sz, sz))\n",
    "            dummy_out = m.eval()(x)\n",
    "            return dummy_out.shape[1]\n",
    "        except Exception as e:\n",
    "            sz *= 2\n",
    "            if sz > 2048: raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sequential(nn.Conv2d(3,5,3), nn.Conv2d(5,11,3))\n",
    "test_eq(num_features_model(m, ch_int=3), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def requires_grad(m):\n",
    "    \"Check if the first parameter of `m` requires grad or not\"\n",
    "    ps = list(m.parameters())\n",
    "    return ps[0].requires_grad if len(ps) > 0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "norm_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d, nn.LayerNorm,)\n",
    "\n",
    "def init_default(m, func=torch.nn.init.kaiming_normal_):\n",
    "    \"Initialize `m` weights with `func` and set `bias` to 0.\"\n",
    "    if func:\n",
    "        if hasattr(m, \"weight\"):\n",
    "            func(m.weight)\n",
    "        if hasattr(m, \"bias\") and hasattr(m.bias, \"data\"):\n",
    "            m.bias.data.fill_(0.0)\n",
    "    return m\n",
    "\n",
    "\n",
    "def cond_init(m, func):\n",
    "    \"Apply `init_default` to `m` unless it's a batchnorm module\"\n",
    "    if (not isinstance(m, norm_types)) and requires_grad(m):\n",
    "        init_default(m, func)\n",
    "\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    \"Apply `f` to children of `m`.\"\n",
    "    c = m.children()\n",
    "    if isinstance(m, nn.Module):\n",
    "        f(m)\n",
    "    for l in c:\n",
    "        apply_leaf(l, f)\n",
    "\n",
    "\n",
    "def apply_init(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize all non-batchnorm layers of `m` with `func`.\"\n",
    "    apply_leaf(m, partial(cond_init, func=func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n",
    "\n",
    "def set_bn_eval(m: Module):\n",
    "    \"Set bn layers in eval mode for all recursive children of `m`.\"\n",
    "    for l in m.children():\n",
    "        if isinstance(l, bn_types) and not next(l.parameters()).requires_grad:\n",
    "            l.eval()\n",
    "            for param in l.parameters(): \n",
    "                param.requires_grad = False\n",
    "        set_bn_eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 00a_core.common.ipynb.\n",
      "Converted 00b_core.data_utils.ipynb.\n",
      "Converted 00c_core.optim.ipynb.\n",
      "Converted 00d_core.schedules.ipynb.\n",
      "Converted 00e_core.layers.ipynb.\n",
      "Converted 01a_classification.data.transforms.ipynb.\n",
      "Converted 01b_classification.data.datasets.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_cv",
   "language": "python",
   "name": "lightning_cv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
